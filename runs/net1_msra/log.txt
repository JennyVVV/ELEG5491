2017-03-21 13:27:14,157 INFO connecting pavi service http://pavi.parrotsdnn.org/log
2017-03-21 13:27:14,201 INFO pavi service connected, instance_id: f3d789aae444421585a0f6e129463966
2017-03-21 13:27:14,223 INFO model name: net1_msra
2017-03-21 13:27:14,224 DEBUG name: "net1_msra"
inputs:
  - { id: data , spec: "Float32(32, 32, 3, _)" }
  - { id: label, spec: "Uint32(1, _)" }
params:
  - { id: conv1.w, spec: "Float32(5, 5, 3, 6)" , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: conv2.w, spec: "Float32(5, 5, 6, 16)", learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.w  , spec: "Float32(400, 120)"   , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.b  , spec: "Float32(120, 1)"     , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc2.w  , spec: "Float32(120, 84)"    , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc2.b  , spec: "Float32(84, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc.w   , spec: "Float32(84, 10)"     , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc.b   , spec: "Float32(10, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
layers:
  - { id: conv1, expr: "conv1 = Convolution(data, @conv1.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 6, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu1, expr: "conv1 = ReLU(conv1)" }
  - { id: pool1, expr: "pool1 = Pooling(conv1)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool1relu, expr: "pool1 = ReLU(pool1)" }
  - { id: conv2, expr: "conv2 = Convolution(pool1, @conv2.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 16, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu2, expr: "conv2 = ReLU(conv2)" }
  - { id: pool2, expr: "pool2 = Pooling(conv2)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool2relu, expr: "pool2 = ReLU(pool2)" }
  - { id: fc1, expr: "fc1 = FullyConnected(pool2, @fc1.w, @fc1.b)",
      attrs: {slice_axis: -2, num_output: 120} }
  - { id: fc1relu, expr: "fc1 = ReLU(fc1)" }
  - { id: fc2, expr: "fc2 = FullyConnected(fc1, @fc2.w, @fc2.b)",
      attrs: {slice_axis: -2, num_output: 84} }
  - { id: fc2relu, expr: "fc2 = ReLU(fc2)" }
  - { id: fc, expr: "fc = FullyConnected(fc2, @fc.w, @fc.b)",
      attrs: {slice_axis: -2, num_output: 10} }
  - { id: loss, expr: "loss = SoftmaxWithLoss(fc, label)",
      attrs: {axis: 0} }
  - { id: accuracy_top1, expr: "accuracy_top1 = Accuracy(fc, label)",
      attrs: {top_k: 1, slice_axis: 0} }
flows:
  - { name: main, inputs: [data, label], outputs: [loss, accuracy_top1], losses: [loss * 1, accuracy_top1 * 1] }

2017-03-21 13:27:15,999 INFO train: iter: 250, lr: 0.001000, ave loss: 2.6914956001
2017-03-21 13:27:15,999 INFO 	loss: 2.691496
2017-03-21 13:27:15,999 INFO 	accuracy_top1: 0.099578
2017-03-21 13:27:16,932 INFO train: iter: 500, lr: 0.001000, ave loss: 2.30258743215
2017-03-21 13:27:16,932 INFO 	loss: 2.302587
2017-03-21 13:27:16,932 INFO 	accuracy_top1: 0.098984
2017-03-21 13:27:17,101 INFO val: iter: 500, lr: 0.001000, ave loss: 2.30260228515
2017-03-21 13:27:17,101 INFO 	loss: 2.302602
2017-03-21 13:27:17,101 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:18,101 INFO train: iter: 750, lr: 0.001000, ave loss: 2.30261948693
2017-03-21 13:27:18,102 INFO 	loss: 2.302619
2017-03-21 13:27:18,102 INFO 	accuracy_top1: 0.099922
2017-03-21 13:27:19,168 INFO train: iter: 1000, lr: 0.001000, ave loss: 2.30262287807
2017-03-21 13:27:19,168 INFO 	loss: 2.302623
2017-03-21 13:27:19,169 INFO 	accuracy_top1: 0.097906
2017-03-21 13:27:19,399 INFO val: iter: 1000, lr: 0.001000, ave loss: 2.30258847773
2017-03-21 13:27:19,399 INFO 	loss: 2.302588
2017-03-21 13:27:19,399 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:20,536 INFO train: iter: 1250, lr: 0.001000, ave loss: 2.30260651135
2017-03-21 13:27:20,536 INFO 	loss: 2.302607
2017-03-21 13:27:20,536 INFO 	accuracy_top1: 0.098578
2017-03-21 13:27:21,495 INFO train: iter: 1500, lr: 0.001000, ave loss: 2.30261408031
2017-03-21 13:27:21,496 INFO 	loss: 2.302614
2017-03-21 13:27:21,496 INFO 	accuracy_top1: 0.100625
2017-03-21 13:27:21,621 INFO val: iter: 1500, lr: 0.001000, ave loss: 2.30258989334
2017-03-21 13:27:21,622 INFO 	loss: 2.302590
2017-03-21 13:27:21,622 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:22,572 INFO train: iter: 1750, lr: 0.001000, ave loss: 2.30260677826
2017-03-21 13:27:22,572 INFO 	loss: 2.302607
2017-03-21 13:27:22,572 INFO 	accuracy_top1: 0.099187
2017-03-21 13:27:23,524 INFO train: iter: 2000, lr: 0.001000, ave loss: 2.30261539018
2017-03-21 13:27:23,524 INFO 	loss: 2.302615
2017-03-21 13:27:23,524 INFO 	accuracy_top1: 0.099219
2017-03-21 13:27:23,659 INFO val: iter: 2000, lr: 0.001000, ave loss: 2.30258946717
2017-03-21 13:27:23,659 INFO 	loss: 2.302589
2017-03-21 13:27:23,659 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:24,613 INFO train: iter: 2250, lr: 0.001000, ave loss: 2.3025888952
2017-03-21 13:27:24,614 INFO 	loss: 2.302589
2017-03-21 13:27:24,614 INFO 	accuracy_top1: 0.100375
2017-03-21 13:27:25,664 INFO train: iter: 2500, lr: 0.001000, ave loss: 2.30261230075
2017-03-21 13:27:25,664 INFO 	loss: 2.302612
2017-03-21 13:27:25,665 INFO 	accuracy_top1: 0.099672
2017-03-21 13:27:25,809 INFO val: iter: 2500, lr: 0.001000, ave loss: 2.30259746313
2017-03-21 13:27:25,810 INFO 	loss: 2.302597
2017-03-21 13:27:25,810 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:26,754 INFO train: iter: 2750, lr: 0.001000, ave loss: 2.30261841643
2017-03-21 13:27:26,754 INFO 	loss: 2.302618
2017-03-21 13:27:26,754 INFO 	accuracy_top1: 0.098469
2017-03-21 13:27:27,681 INFO train: iter: 3000, lr: 0.001000, ave loss: 2.30260985863
2017-03-21 13:27:27,681 INFO 	loss: 2.302610
2017-03-21 13:27:27,681 INFO 	accuracy_top1: 0.099797
2017-03-21 13:27:27,843 INFO val: iter: 3000, lr: 0.001000, ave loss: 2.30259666443
2017-03-21 13:27:27,844 INFO 	loss: 2.302597
2017-03-21 13:27:27,844 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:28,864 INFO train: iter: 3250, lr: 0.001000, ave loss: 2.30263150322
2017-03-21 13:27:28,864 INFO 	loss: 2.302632
2017-03-21 13:27:28,864 INFO 	accuracy_top1: 0.096266
2017-03-21 13:27:29,831 INFO train: iter: 3500, lr: 0.001000, ave loss: 2.30259116614
2017-03-21 13:27:29,832 INFO 	loss: 2.302591
2017-03-21 13:27:29,832 INFO 	accuracy_top1: 0.099562
2017-03-21 13:27:29,979 INFO val: iter: 3500, lr: 0.001000, ave loss: 2.30259777308
2017-03-21 13:27:29,979 INFO 	loss: 2.302598
2017-03-21 13:27:29,980 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:30,899 INFO train: iter: 3750, lr: 0.001000, ave loss: 2.30262194216
2017-03-21 13:27:30,900 INFO 	loss: 2.302622
2017-03-21 13:27:30,900 INFO 	accuracy_top1: 0.100422
2017-03-21 13:27:31,939 INFO train: iter: 4000, lr: 0.001000, ave loss: 2.30259213042
2017-03-21 13:27:31,940 INFO 	loss: 2.302592
2017-03-21 13:27:31,940 INFO 	accuracy_top1: 0.099937
2017-03-21 13:27:32,104 INFO val: iter: 4000, lr: 0.001000, ave loss: 2.30259815454
2017-03-21 13:27:32,104 INFO 	loss: 2.302598
2017-03-21 13:27:32,104 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:33,179 INFO train: iter: 4250, lr: 0.001000, ave loss: 2.30261918163
2017-03-21 13:27:33,180 INFO 	loss: 2.302619
2017-03-21 13:27:33,180 INFO 	accuracy_top1: 0.098578
2017-03-21 13:27:34,196 INFO train: iter: 4500, lr: 0.001000, ave loss: 2.30261072135
2017-03-21 13:27:34,196 INFO 	loss: 2.302611
2017-03-21 13:27:34,197 INFO 	accuracy_top1: 0.099156
2017-03-21 13:27:34,351 INFO val: iter: 4500, lr: 0.001000, ave loss: 2.30259284973
2017-03-21 13:27:34,352 INFO 	loss: 2.302593
2017-03-21 13:27:34,352 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:35,340 INFO train: iter: 4750, lr: 0.001000, ave loss: 2.30261609936
2017-03-21 13:27:35,340 INFO 	loss: 2.302616
2017-03-21 13:27:35,340 INFO 	accuracy_top1: 0.099141
2017-03-21 13:27:36,449 INFO train: iter: 5000, lr: 0.001000, ave loss: 2.30261095047
2017-03-21 13:27:36,449 INFO 	loss: 2.302611
2017-03-21 13:27:36,450 INFO 	accuracy_top1: 0.099297
2017-03-21 13:27:36,596 INFO val: iter: 5000, lr: 0.001000, ave loss: 2.30259371102
2017-03-21 13:27:36,596 INFO 	loss: 2.302594
2017-03-21 13:27:36,596 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:37,674 INFO train: iter: 5250, lr: 0.001000, ave loss: 2.30262395275
2017-03-21 13:27:37,674 INFO 	loss: 2.302624
2017-03-21 13:27:37,674 INFO 	accuracy_top1: 0.098219
2017-03-21 13:27:38,600 INFO train: iter: 5500, lr: 0.001000, ave loss: 2.30260301042
2017-03-21 13:27:38,600 INFO 	loss: 2.302603
2017-03-21 13:27:38,600 INFO 	accuracy_top1: 0.097172
2017-03-21 13:27:38,740 INFO val: iter: 5500, lr: 0.001000, ave loss: 2.30259225368
2017-03-21 13:27:38,740 INFO 	loss: 2.302592
2017-03-21 13:27:38,740 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:39,640 INFO train: iter: 5750, lr: 0.001000, ave loss: 2.30261013579
2017-03-21 13:27:39,641 INFO 	loss: 2.302610
2017-03-21 13:27:39,641 INFO 	accuracy_top1: 0.100687
2017-03-21 13:27:40,591 INFO train: iter: 6000, lr: 0.001000, ave loss: 2.30262083125
2017-03-21 13:27:40,591 INFO 	loss: 2.302621
2017-03-21 13:27:40,591 INFO 	accuracy_top1: 0.099156
2017-03-21 13:27:40,746 INFO val: iter: 6000, lr: 0.001000, ave loss: 2.30259134769
2017-03-21 13:27:40,746 INFO 	loss: 2.302591
2017-03-21 13:27:40,746 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:41,757 INFO train: iter: 6250, lr: 0.001000, ave loss: 2.302587551
2017-03-21 13:27:41,757 INFO 	loss: 2.302588
2017-03-21 13:27:41,757 INFO 	accuracy_top1: 0.101625
2017-03-21 13:27:42,763 INFO train: iter: 6500, lr: 0.001000, ave loss: 2.302620327
2017-03-21 13:27:42,764 INFO 	loss: 2.302620
2017-03-21 13:27:42,764 INFO 	accuracy_top1: 0.099797
2017-03-21 13:27:42,929 INFO val: iter: 6500, lr: 0.001000, ave loss: 2.30259495974
2017-03-21 13:27:42,929 INFO 	loss: 2.302595
2017-03-21 13:27:42,929 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:43,914 INFO train: iter: 6750, lr: 0.001000, ave loss: 2.30260702026
2017-03-21 13:27:43,914 INFO 	loss: 2.302607
2017-03-21 13:27:43,914 INFO 	accuracy_top1: 0.099281
2017-03-21 13:27:44,901 INFO train: iter: 7000, lr: 0.001000, ave loss: 2.30259382796
2017-03-21 13:27:44,901 INFO 	loss: 2.302594
2017-03-21 13:27:44,901 INFO 	accuracy_top1: 0.098844
2017-03-21 13:27:45,024 INFO val: iter: 7000, lr: 0.001000, ave loss: 2.30260177851
2017-03-21 13:27:45,024 INFO 	loss: 2.302602
2017-03-21 13:27:45,024 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:45,990 INFO train: iter: 7250, lr: 0.001000, ave loss: 2.30262362695
2017-03-21 13:27:45,991 INFO 	loss: 2.302624
2017-03-21 13:27:45,991 INFO 	accuracy_top1: 0.100031
2017-03-21 13:27:46,891 INFO train: iter: 7500, lr: 0.001000, ave loss: 2.30259155071
2017-03-21 13:27:46,891 INFO 	loss: 2.302592
2017-03-21 13:27:46,892 INFO 	accuracy_top1: 0.098828
2017-03-21 13:27:47,031 INFO val: iter: 7500, lr: 0.001000, ave loss: 2.30260148048
2017-03-21 13:27:47,032 INFO 	loss: 2.302601
2017-03-21 13:27:47,032 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:48,020 INFO train: iter: 7750, lr: 0.001000, ave loss: 2.30262306678
2017-03-21 13:27:48,020 INFO 	loss: 2.302623
2017-03-21 13:27:48,020 INFO 	accuracy_top1: 0.099266
2017-03-21 13:27:48,930 INFO train: iter: 8000, lr: 0.000100, ave loss: 2.30260644341
2017-03-21 13:27:48,930 INFO 	loss: 2.302606
2017-03-21 13:27:48,931 INFO 	accuracy_top1: 0.099156
2017-03-21 13:27:49,084 INFO val: iter: 8000, lr: 0.000100, ave loss: 2.30259735286
2017-03-21 13:27:49,084 INFO 	loss: 2.302597
2017-03-21 13:27:49,084 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:50,030 INFO train: iter: 8250, lr: 0.000100, ave loss: 2.3026243968
2017-03-21 13:27:50,030 INFO 	loss: 2.302624
2017-03-21 13:27:50,030 INFO 	accuracy_top1: 0.099062
2017-03-21 13:27:51,016 INFO train: iter: 8500, lr: 0.000100, ave loss: 2.30258419943
2017-03-21 13:27:51,016 INFO 	loss: 2.302584
2017-03-21 13:27:51,016 INFO 	accuracy_top1: 0.099562
2017-03-21 13:27:51,169 INFO val: iter: 8500, lr: 0.000100, ave loss: 2.30259761512
2017-03-21 13:27:51,170 INFO 	loss: 2.302598
2017-03-21 13:27:51,170 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:52,221 INFO train: iter: 8750, lr: 0.000100, ave loss: 2.30260274458
2017-03-21 13:27:52,221 INFO 	loss: 2.302603
2017-03-21 13:27:52,221 INFO 	accuracy_top1: 0.099328
2017-03-21 13:27:53,256 INFO train: iter: 9000, lr: 0.000100, ave loss: 2.30259485412
2017-03-21 13:27:53,256 INFO 	loss: 2.302595
2017-03-21 13:27:53,257 INFO 	accuracy_top1: 0.100156
2017-03-21 13:27:53,407 INFO val: iter: 9000, lr: 0.000100, ave loss: 2.30259557962
2017-03-21 13:27:53,407 INFO 	loss: 2.302596
2017-03-21 13:27:53,407 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:54,364 INFO train: iter: 9250, lr: 0.000100, ave loss: 2.30261040831
2017-03-21 13:27:54,364 INFO 	loss: 2.302610
2017-03-21 13:27:54,365 INFO 	accuracy_top1: 0.100406
2017-03-21 13:27:55,295 INFO train: iter: 9500, lr: 0.000100, ave loss: 2.30260079837
2017-03-21 13:27:55,296 INFO 	loss: 2.302601
2017-03-21 13:27:55,296 INFO 	accuracy_top1: 0.098891
2017-03-21 13:27:55,452 INFO val: iter: 9500, lr: 0.000100, ave loss: 2.30259276927
2017-03-21 13:27:55,452 INFO 	loss: 2.302593
2017-03-21 13:27:55,452 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:56,395 INFO train: iter: 9750, lr: 0.000100, ave loss: 2.30262401569
2017-03-21 13:27:56,395 INFO 	loss: 2.302624
2017-03-21 13:27:56,395 INFO 	accuracy_top1: 0.099516
2017-03-21 13:27:57,284 INFO train: iter: 10000, lr: 0.000010, ave loss: 2.30258361459
2017-03-21 13:27:57,284 INFO 	loss: 2.302584
2017-03-21 13:27:57,284 INFO 	accuracy_top1: 0.099906
2017-03-21 13:27:57,418 INFO val: iter: 10000, lr: 0.000010, ave loss: 2.30259064436
2017-03-21 13:27:57,419 INFO 	loss: 2.302591
2017-03-21 13:27:57,419 INFO 	accuracy_top1: 0.100000
2017-03-21 13:27:58,222 ERROR KeyboardInterrupt
Traceback (most recent call last):
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/runner.py", line 177, in run
    for logs in policy.run():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/basepolicy.py", line 19, in run
    for logs in self.iter():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/iterate.py", line 47, in iter
    self.flow.iterate()
KeyboardInterrupt
2017-03-21 13:28:16,126 INFO connecting pavi service http://pavi.parrotsdnn.org/log
2017-03-21 13:28:16,160 INFO pavi service connected, instance_id: ff17b0b2417f4c7db3a665b8b246b39e
2017-03-21 13:28:16,183 INFO model name: net1_msra
2017-03-21 13:28:16,183 DEBUG name: "net1_msra"
inputs:
  - { id: data , spec: "Float32(32, 32, 3, _)" }
  - { id: label, spec: "Uint32(1, _)" }
params:
  - { id: conv1.w, spec: "Float32(5, 5, 3, 6)" , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: conv2.w, spec: "Float32(5, 5, 6, 16)", learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.w  , spec: "Float32(400, 120)"   , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.b  , spec: "Float32(120, 1)"     , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc2.w  , spec: "Float32(120, 84)"    , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc2.b  , spec: "Float32(84, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc.w   , spec: "Float32(84, 10)"     , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc.b   , spec: "Float32(10, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
layers:
  - { id: conv1, expr: "conv1 = Convolution(data, @conv1.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 6, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu1, expr: "conv1 = ReLU(conv1)" }
  - { id: pool1, expr: "pool1 = Pooling(conv1)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool1relu, expr: "pool1 = ReLU(pool1)" }
  - { id: conv2, expr: "conv2 = Convolution(pool1, @conv2.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 16, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu2, expr: "conv2 = ReLU(conv2)" }
  - { id: pool2, expr: "pool2 = Pooling(conv2)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool2relu, expr: "pool2 = ReLU(pool2)" }
  - { id: fc1, expr: "fc1 = FullyConnected(pool2, @fc1.w, @fc1.b)",
      attrs: {slice_axis: -2, num_output: 120} }
  - { id: fc1relu, expr: "fc1 = ReLU(fc1)" }
  - { id: fc2, expr: "fc2 = FullyConnected(fc1, @fc2.w, @fc2.b)",
      attrs: {slice_axis: -2, num_output: 84} }
  - { id: fc2relu, expr: "fc2 = ReLU(fc2)" }
  - { id: fc, expr: "fc = FullyConnected(fc2, @fc.w, @fc.b)",
      attrs: {slice_axis: -2, num_output: 10} }
  - { id: loss, expr: "loss = SoftmaxWithLoss(fc, label)",
      attrs: {axis: 0} }
  - { id: accuracy_top1, expr: "accuracy_top1 = Accuracy(fc, label)",
      attrs: {top_k: 1, slice_axis: 0} }
flows:
  - { name: main, inputs: [data, label], outputs: [loss, accuracy_top1], losses: [loss * 1, accuracy_top1 * 1] }

2017-03-21 13:28:18,100 INFO train: iter: 250, lr: 0.001000, ave loss: 2.68977560484
2017-03-21 13:28:18,100 INFO 	loss: 2.689776
2017-03-21 13:28:18,101 INFO 	accuracy_top1: 0.100922
2017-03-21 13:28:19,078 INFO train: iter: 500, lr: 0.001000, ave loss: 2.30261255848
2017-03-21 13:28:19,078 INFO 	loss: 2.302613
2017-03-21 13:28:19,079 INFO 	accuracy_top1: 0.099437
2017-03-21 13:28:19,267 INFO val: iter: 500, lr: 0.001000, ave loss: 2.30260411203
2017-03-21 13:28:19,267 INFO 	loss: 2.302604
2017-03-21 13:28:19,267 INFO 	accuracy_top1: nan
2017-03-21 13:28:20,238 INFO train: iter: 750, lr: 0.001000, ave loss: 2.30262820768
2017-03-21 13:28:20,238 INFO 	loss: 2.302628
2017-03-21 13:28:20,238 INFO 	accuracy_top1: 0.099187
2017-03-21 13:28:21,208 INFO train: iter: 1000, lr: 0.001000, ave loss: 2.30260239983
2017-03-21 13:28:21,208 INFO 	loss: 2.302602
2017-03-21 13:28:21,208 INFO 	accuracy_top1: 0.098562
2017-03-21 13:28:21,367 INFO val: iter: 1000, lr: 0.001000, ave loss: 2.30259863138
2017-03-21 13:28:21,367 INFO 	loss: 2.302599
2017-03-21 13:28:21,367 INFO 	accuracy_top1: nan
2017-03-21 13:28:22,375 INFO train: iter: 1250, lr: 0.001000, ave loss: 2.30258569074
2017-03-21 13:28:22,375 INFO 	loss: 2.302586
2017-03-21 13:28:22,375 INFO 	accuracy_top1: 0.100250
2017-03-21 13:28:22,966 ERROR KeyboardInterrupt
Traceback (most recent call last):
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/runner.py", line 177, in run
    for logs in policy.run():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/basepolicy.py", line 19, in run
    for logs in self.iter():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/iterate.py", line 47, in iter
    self.flow.iterate()
KeyboardInterrupt
2017-03-21 13:28:36,664 INFO connecting pavi service http://pavi.parrotsdnn.org/log
2017-03-21 13:28:36,693 INFO pavi service connected, instance_id: 9a67b201ce3c4159ae1be3368613f461
2017-03-21 13:28:36,707 INFO model name: net1_msra
2017-03-21 13:28:36,708 DEBUG name: "net1_msra"
inputs:
  - { id: data , spec: "Float32(32, 32, 3, _)" }
  - { id: label, spec: "Uint32(1, _)" }
params:
  - { id: conv1.w, spec: "Float32(5, 5, 3, 6)" , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: conv2.w, spec: "Float32(5, 5, 6, 16)", learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.w  , spec: "Float32(400, 120)"   , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.b  , spec: "Float32(120, 1)"     , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc2.w  , spec: "Float32(120, 84)"    , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc2.b  , spec: "Float32(84, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc.w   , spec: "Float32(84, 10)"     , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc.b   , spec: "Float32(10, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
layers:
  - { id: conv1, expr: "conv1 = Convolution(data, @conv1.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 6, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu1, expr: "conv1 = ReLU(conv1)" }
  - { id: pool1, expr: "pool1 = Pooling(conv1)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool1relu, expr: "pool1 = ReLU(pool1)" }
  - { id: conv2, expr: "conv2 = Convolution(pool1, @conv2.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 16, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu2, expr: "conv2 = ReLU(conv2)" }
  - { id: pool2, expr: "pool2 = Pooling(conv2)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool2relu, expr: "pool2 = ReLU(pool2)" }
  - { id: fc1, expr: "fc1 = FullyConnected(pool2, @fc1.w, @fc1.b)",
      attrs: {slice_axis: -2, num_output: 120} }
  - { id: fc1relu, expr: "fc1 = ReLU(fc1)" }
  - { id: fc2, expr: "fc2 = FullyConnected(fc1, @fc2.w, @fc2.b)",
      attrs: {slice_axis: -2, num_output: 84} }
  - { id: fc2relu, expr: "fc2 = ReLU(fc2)" }
  - { id: fc, expr: "fc = FullyConnected(fc2, @fc.w, @fc.b)",
      attrs: {slice_axis: -2, num_output: 10} }
  - { id: loss, expr: "loss = SoftmaxWithLoss(fc, label)",
      attrs: {axis: 0} }
  - { id: accuracy_top1, expr: "accuracy_top1 = Accuracy(fc, label)",
      attrs: {top_k: 1, slice_axis: 0} }
flows:
  - { name: main, inputs: [data, label], outputs: [loss, accuracy_top1], losses: [loss * 1, accuracy_top1 * 1] }

2017-03-21 13:28:37,973 INFO train: iter: 250, lr: 0.001000, ave loss: 2.63305945587
2017-03-21 13:28:37,974 INFO 	loss: 2.633059
2017-03-21 13:28:37,974 INFO 	accuracy_top1: 0.098531
2017-03-21 13:28:38,975 INFO train: iter: 500, lr: 0.001000, ave loss: 2.30260877669
2017-03-21 13:28:38,976 INFO 	loss: 2.302609
2017-03-21 13:28:38,976 INFO 	accuracy_top1: 0.099328
2017-03-21 13:28:39,134 INFO val: iter: 500, lr: 0.001000, ave loss: 2.30259474218
2017-03-21 13:28:39,134 INFO 	loss: 2.302595
2017-03-21 13:28:39,134 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:40,277 INFO train: iter: 750, lr: 0.001000, ave loss: 2.30261732543
2017-03-21 13:28:40,278 INFO 	loss: 2.302617
2017-03-21 13:28:40,278 INFO 	accuracy_top1: 0.098766
2017-03-21 13:28:41,225 INFO train: iter: 1000, lr: 0.001000, ave loss: 2.3026117866
2017-03-21 13:28:41,225 INFO 	loss: 2.302612
2017-03-21 13:28:41,225 INFO 	accuracy_top1: 0.099500
2017-03-21 13:28:41,407 INFO val: iter: 1000, lr: 0.001000, ave loss: 2.30259053409
2017-03-21 13:28:41,407 INFO 	loss: 2.302591
2017-03-21 13:28:41,407 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:42,414 INFO train: iter: 1250, lr: 0.001000, ave loss: 2.30260680938
2017-03-21 13:28:42,414 INFO 	loss: 2.302607
2017-03-21 13:28:42,414 INFO 	accuracy_top1: 0.099250
2017-03-21 13:28:43,384 INFO train: iter: 1500, lr: 0.001000, ave loss: 2.30259141171
2017-03-21 13:28:43,385 INFO 	loss: 2.302591
2017-03-21 13:28:43,385 INFO 	accuracy_top1: 0.098094
2017-03-21 13:28:43,520 INFO val: iter: 1500, lr: 0.001000, ave loss: 2.30259912312
2017-03-21 13:28:43,521 INFO 	loss: 2.302599
2017-03-21 13:28:43,521 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:44,482 INFO train: iter: 1750, lr: 0.001000, ave loss: 2.30263839412
2017-03-21 13:28:44,482 INFO 	loss: 2.302638
2017-03-21 13:28:44,482 INFO 	accuracy_top1: 0.098281
2017-03-21 13:28:45,451 INFO train: iter: 2000, lr: 0.001000, ave loss: 2.30261544812
2017-03-21 13:28:45,452 INFO 	loss: 2.302615
2017-03-21 13:28:45,452 INFO 	accuracy_top1: 0.099234
2017-03-21 13:28:45,589 INFO val: iter: 2000, lr: 0.001000, ave loss: 2.30258750618
2017-03-21 13:28:45,589 INFO 	loss: 2.302588
2017-03-21 13:28:45,590 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:46,464 INFO train: iter: 2250, lr: 0.001000, ave loss: 2.30259754276
2017-03-21 13:28:46,464 INFO 	loss: 2.302598
2017-03-21 13:28:46,464 INFO 	accuracy_top1: 0.101078
2017-03-21 13:28:47,386 INFO train: iter: 2500, lr: 0.001000, ave loss: 2.30260757875
2017-03-21 13:28:47,387 INFO 	loss: 2.302608
2017-03-21 13:28:47,387 INFO 	accuracy_top1: 0.100125
2017-03-21 13:28:47,530 INFO val: iter: 2500, lr: 0.001000, ave loss: 2.30259382427
2017-03-21 13:28:47,531 INFO 	loss: 2.302594
2017-03-21 13:28:47,531 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:48,549 INFO train: iter: 2750, lr: 0.001000, ave loss: 2.30259744942
2017-03-21 13:28:48,550 INFO 	loss: 2.302597
2017-03-21 13:28:48,550 INFO 	accuracy_top1: 0.098109
2017-03-21 13:28:49,558 INFO train: iter: 3000, lr: 0.001000, ave loss: 2.3026010648
2017-03-21 13:28:49,559 INFO 	loss: 2.302601
2017-03-21 13:28:49,559 INFO 	accuracy_top1: 0.101109
2017-03-21 13:28:49,740 INFO val: iter: 3000, lr: 0.001000, ave loss: 2.30260253251
2017-03-21 13:28:49,741 INFO 	loss: 2.302603
2017-03-21 13:28:49,741 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:50,843 INFO train: iter: 3250, lr: 0.001000, ave loss: 2.30261860526
2017-03-21 13:28:50,844 INFO 	loss: 2.302619
2017-03-21 13:28:50,845 INFO 	accuracy_top1: 0.100500
2017-03-21 13:28:51,883 INFO train: iter: 3500, lr: 0.001000, ave loss: 2.30261566889
2017-03-21 13:28:51,884 INFO 	loss: 2.302616
2017-03-21 13:28:51,884 INFO 	accuracy_top1: 0.099031
2017-03-21 13:28:52,008 INFO val: iter: 3500, lr: 0.001000, ave loss: 2.30259272754
2017-03-21 13:28:52,008 INFO 	loss: 2.302593
2017-03-21 13:28:52,008 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:52,880 INFO train: iter: 3750, lr: 0.001000, ave loss: 2.30261389506
2017-03-21 13:28:52,881 INFO 	loss: 2.302614
2017-03-21 13:28:52,881 INFO 	accuracy_top1: 0.099625
2017-03-21 13:28:53,850 INFO train: iter: 4000, lr: 0.001000, ave loss: 2.30260632551
2017-03-21 13:28:53,851 INFO 	loss: 2.302606
2017-03-21 13:28:53,851 INFO 	accuracy_top1: 0.099406
2017-03-21 13:28:53,987 INFO val: iter: 4000, lr: 0.001000, ave loss: 2.3025965929
2017-03-21 13:28:53,988 INFO 	loss: 2.302597
2017-03-21 13:28:53,988 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:55,012 INFO train: iter: 4250, lr: 0.001000, ave loss: 2.30260322118
2017-03-21 13:28:55,012 INFO 	loss: 2.302603
2017-03-21 13:28:55,013 INFO 	accuracy_top1: 0.099750
2017-03-21 13:28:55,975 INFO train: iter: 4500, lr: 0.001000, ave loss: 2.30257008564
2017-03-21 13:28:55,975 INFO 	loss: 2.302570
2017-03-21 13:28:55,975 INFO 	accuracy_top1: 0.098656
2017-03-21 13:28:56,139 INFO val: iter: 4500, lr: 0.001000, ave loss: 2.30261617005
2017-03-21 13:28:56,140 INFO 	loss: 2.302616
2017-03-21 13:28:56,140 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:57,174 INFO train: iter: 4750, lr: 0.001000, ave loss: 2.30263754845
2017-03-21 13:28:57,174 INFO 	loss: 2.302638
2017-03-21 13:28:57,175 INFO 	accuracy_top1: 0.098875
2017-03-21 13:28:58,030 INFO train: iter: 5000, lr: 0.001000, ave loss: 2.30263707113
2017-03-21 13:28:58,030 INFO 	loss: 2.302637
2017-03-21 13:28:58,030 INFO 	accuracy_top1: 0.098016
2017-03-21 13:28:58,170 INFO val: iter: 5000, lr: 0.001000, ave loss: 2.30258752108
2017-03-21 13:28:58,170 INFO 	loss: 2.302588
2017-03-21 13:28:58,170 INFO 	accuracy_top1: 0.100000
2017-03-21 13:28:59,199 INFO train: iter: 5250, lr: 0.001000, ave loss: 2.30259050441
2017-03-21 13:28:59,199 INFO 	loss: 2.302591
2017-03-21 13:28:59,199 INFO 	accuracy_top1: 0.099953
2017-03-21 13:29:00,199 INFO train: iter: 5500, lr: 0.001000, ave loss: 2.30262781489
2017-03-21 13:29:00,199 INFO 	loss: 2.302628
2017-03-21 13:29:00,199 INFO 	accuracy_top1: 0.099078
2017-03-21 13:29:00,352 INFO val: iter: 5500, lr: 0.001000, ave loss: 2.30258926451
2017-03-21 13:29:00,353 INFO 	loss: 2.302589
2017-03-21 13:29:00,353 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:01,275 INFO train: iter: 5750, lr: 0.001000, ave loss: 2.30260074413
2017-03-21 13:29:01,275 INFO 	loss: 2.302601
2017-03-21 13:29:01,275 INFO 	accuracy_top1: 0.098094
2017-03-21 13:29:02,240 INFO train: iter: 6000, lr: 0.001000, ave loss: 2.3025900712
2017-03-21 13:29:02,241 INFO 	loss: 2.302590
2017-03-21 13:29:02,241 INFO 	accuracy_top1: 0.100797
2017-03-21 13:29:02,387 INFO val: iter: 6000, lr: 0.001000, ave loss: 2.30260358751
2017-03-21 13:29:02,388 INFO 	loss: 2.302604
2017-03-21 13:29:02,388 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:03,457 INFO train: iter: 6250, lr: 0.001000, ave loss: 2.3026387862
2017-03-21 13:29:03,457 INFO 	loss: 2.302639
2017-03-21 13:29:03,457 INFO 	accuracy_top1: 0.098359
2017-03-21 13:29:04,376 INFO train: iter: 6500, lr: 0.001000, ave loss: 2.30261796784
2017-03-21 13:29:04,377 INFO 	loss: 2.302618
2017-03-21 13:29:04,377 INFO 	accuracy_top1: 0.097781
2017-03-21 13:29:04,512 INFO val: iter: 6500, lr: 0.001000, ave loss: 2.3025898695
2017-03-21 13:29:04,513 INFO 	loss: 2.302590
2017-03-21 13:29:04,513 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:05,487 INFO train: iter: 6750, lr: 0.001000, ave loss: 2.30259416628
2017-03-21 13:29:05,487 INFO 	loss: 2.302594
2017-03-21 13:29:05,487 INFO 	accuracy_top1: 0.098531
2017-03-21 13:29:06,394 INFO train: iter: 7000, lr: 0.001000, ave loss: 2.30261251092
2017-03-21 13:29:06,394 INFO 	loss: 2.302613
2017-03-21 13:29:06,394 INFO 	accuracy_top1: 0.098781
2017-03-21 13:29:06,524 INFO val: iter: 7000, lr: 0.001000, ave loss: 2.30259234905
2017-03-21 13:29:06,524 INFO 	loss: 2.302592
2017-03-21 13:29:06,524 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:07,469 INFO train: iter: 7250, lr: 0.001000, ave loss: 2.30261968958
2017-03-21 13:29:07,469 INFO 	loss: 2.302620
2017-03-21 13:29:07,469 INFO 	accuracy_top1: 0.101656
2017-03-21 13:29:08,421 INFO train: iter: 7500, lr: 0.001000, ave loss: 2.30259871757
2017-03-21 13:29:08,421 INFO 	loss: 2.302599
2017-03-21 13:29:08,421 INFO 	accuracy_top1: 0.100922
2017-03-21 13:29:08,560 INFO val: iter: 7500, lr: 0.001000, ave loss: 2.30259774029
2017-03-21 13:29:08,560 INFO 	loss: 2.302598
2017-03-21 13:29:08,560 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:09,531 INFO train: iter: 7750, lr: 0.001000, ave loss: 2.30261469865
2017-03-21 13:29:09,531 INFO 	loss: 2.302615
2017-03-21 13:29:09,532 INFO 	accuracy_top1: 0.098328
2017-03-21 13:29:10,474 INFO train: iter: 8000, lr: 0.000100, ave loss: 2.30260114431
2017-03-21 13:29:10,474 INFO 	loss: 2.302601
2017-03-21 13:29:10,474 INFO 	accuracy_top1: 0.100062
2017-03-21 13:29:10,618 INFO val: iter: 8000, lr: 0.000100, ave loss: 2.30259861946
2017-03-21 13:29:10,618 INFO 	loss: 2.302599
2017-03-21 13:29:10,618 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:11,661 INFO train: iter: 8250, lr: 0.000100, ave loss: 2.30257399487
2017-03-21 13:29:11,661 INFO 	loss: 2.302574
2017-03-21 13:29:11,662 INFO 	accuracy_top1: 0.101484
2017-03-21 13:29:12,629 INFO train: iter: 8500, lr: 0.000100, ave loss: 2.30264540029
2017-03-21 13:29:12,629 INFO 	loss: 2.302645
2017-03-21 13:29:12,629 INFO 	accuracy_top1: 0.099375
2017-03-21 13:29:12,774 INFO val: iter: 8500, lr: 0.000100, ave loss: 2.30259831548
2017-03-21 13:29:12,774 INFO 	loss: 2.302598
2017-03-21 13:29:12,774 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:13,739 INFO train: iter: 8750, lr: 0.000100, ave loss: 2.30259851301
2017-03-21 13:29:13,740 INFO 	loss: 2.302599
2017-03-21 13:29:13,740 INFO 	accuracy_top1: 0.098484
2017-03-21 13:29:14,650 INFO train: iter: 9000, lr: 0.000100, ave loss: 2.30260282362
2017-03-21 13:29:14,650 INFO 	loss: 2.302603
2017-03-21 13:29:14,650 INFO 	accuracy_top1: 0.100406
2017-03-21 13:29:14,778 INFO val: iter: 9000, lr: 0.000100, ave loss: 2.30259599388
2017-03-21 13:29:14,779 INFO 	loss: 2.302596
2017-03-21 13:29:14,779 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:15,879 INFO train: iter: 9250, lr: 0.000100, ave loss: 2.3026145283
2017-03-21 13:29:15,879 INFO 	loss: 2.302615
2017-03-21 13:29:15,879 INFO 	accuracy_top1: 0.098766
2017-03-21 13:29:16,973 INFO train: iter: 9500, lr: 0.000100, ave loss: 2.30260624993
2017-03-21 13:29:16,974 INFO 	loss: 2.302606
2017-03-21 13:29:16,974 INFO 	accuracy_top1: 0.100984
2017-03-21 13:29:17,148 INFO val: iter: 9500, lr: 0.000100, ave loss: 2.30259259939
2017-03-21 13:29:17,148 INFO 	loss: 2.302593
2017-03-21 13:29:17,148 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:18,181 INFO train: iter: 9750, lr: 0.000100, ave loss: 2.30260203063
2017-03-21 13:29:18,181 INFO 	loss: 2.302602
2017-03-21 13:29:18,181 INFO 	accuracy_top1: 0.098219
2017-03-21 13:29:19,137 INFO train: iter: 10000, lr: 0.000010, ave loss: 2.30259681249
2017-03-21 13:29:19,138 INFO 	loss: 2.302597
2017-03-21 13:29:19,138 INFO 	accuracy_top1: 0.100234
2017-03-21 13:29:19,279 INFO val: iter: 10000, lr: 0.000010, ave loss: 2.30259127915
2017-03-21 13:29:19,280 INFO 	loss: 2.302591
2017-03-21 13:29:19,280 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:20,357 INFO train: iter: 10250, lr: 0.000010, ave loss: 2.30257688212
2017-03-21 13:29:20,357 INFO 	loss: 2.302577
2017-03-21 13:29:20,357 INFO 	accuracy_top1: 0.103125
2017-03-21 13:29:21,443 INFO train: iter: 10500, lr: 0.000010, ave loss: 2.30260928345
2017-03-21 13:29:21,443 INFO 	loss: 2.302609
2017-03-21 13:29:21,444 INFO 	accuracy_top1: 0.097547
2017-03-21 13:29:21,588 INFO val: iter: 10500, lr: 0.000010, ave loss: 2.30259083211
2017-03-21 13:29:21,589 INFO 	loss: 2.302591
2017-03-21 13:29:21,589 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:22,728 INFO train: iter: 10750, lr: 0.000010, ave loss: 2.30260966277
2017-03-21 13:29:22,729 INFO 	loss: 2.302610
2017-03-21 13:29:22,729 INFO 	accuracy_top1: 0.101078
2017-03-21 13:29:23,755 INFO train: iter: 11000, lr: 0.000010, ave loss: 2.30258909261
2017-03-21 13:29:23,756 INFO 	loss: 2.302589
2017-03-21 13:29:23,756 INFO 	accuracy_top1: 0.098891
2017-03-21 13:29:23,902 INFO val: iter: 11000, lr: 0.000010, ave loss: 2.30259055197
2017-03-21 13:29:23,903 INFO 	loss: 2.302591
2017-03-21 13:29:23,903 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:24,854 INFO train: iter: 11250, lr: 0.000010, ave loss: 2.30260987926
2017-03-21 13:29:24,854 INFO 	loss: 2.302610
2017-03-21 13:29:24,854 INFO 	accuracy_top1: 0.098547
2017-03-21 13:29:25,825 INFO train: iter: 11500, lr: 0.000010, ave loss: 2.30256720984
2017-03-21 13:29:25,826 INFO 	loss: 2.302567
2017-03-21 13:29:25,826 INFO 	accuracy_top1: 0.102094
2017-03-21 13:29:25,957 INFO val: iter: 11500, lr: 0.000010, ave loss: 2.30259057283
2017-03-21 13:29:25,957 INFO 	loss: 2.302591
2017-03-21 13:29:25,957 INFO 	accuracy_top1: 0.100000
2017-03-21 13:29:26,848 INFO train: iter: 11750, lr: 0.000010, ave loss: 2.3025797596
2017-03-21 13:29:26,848 INFO 	loss: 2.302580
2017-03-21 13:29:26,848 INFO 	accuracy_top1: 0.098203
2017-03-21 13:29:27,819 INFO train: iter: 12000, lr: 0.000010, ave loss: 2.30258595133
2017-03-21 13:29:27,819 INFO 	loss: 2.302586
2017-03-21 13:29:27,819 INFO 	accuracy_top1: 0.100594
2017-03-21 13:29:27,987 INFO val: iter: 12000, lr: 0.000010, ave loss: 2.30259057879
2017-03-21 13:29:27,987 INFO 	loss: 2.302591
2017-03-21 13:29:27,988 INFO 	accuracy_top1: 0.100000
2017-03-21 13:30:24,134 INFO connecting pavi service http://pavi.parrotsdnn.org/log
2017-03-21 13:30:24,175 INFO pavi service connected, instance_id: 6fda9e64e3a24d3f96fde7a15723d34f
2017-03-21 13:30:24,196 INFO model name: net1_msra
2017-03-21 13:30:24,197 DEBUG name: "net1_msra"
inputs:
  - { id: data , spec: "Float32(32, 32, 3, _)" }
  - { id: label, spec: "Uint32(1, _)" }
params:
  - { id: conv1.w, spec: "Float32(5, 5, 3, 6)" , learning-policy: { init: gauss(0.01), lr_mult: 1, decay_mult: 1 } }
  - { id: conv2.w, spec: "Float32(5, 5, 6, 16)", learning-policy: { init: gauss(0.01), lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.w  , spec: "Float32(400, 120)"   , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.b  , spec: "Float32(120, 1)"     , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc2.w  , spec: "Float32(120, 84)"    , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc2.b  , spec: "Float32(84, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc.w   , spec: "Float32(84, 10)"     , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc.b   , spec: "Float32(10, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
layers:
  - { id: conv1, expr: "conv1 = Convolution(data, @conv1.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 6, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu1, expr: "conv1 = ReLU(conv1)" }
  - { id: pool1, expr: "pool1 = Pooling(conv1)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool1relu, expr: "pool1 = ReLU(pool1)" }
  - { id: conv2, expr: "conv2 = Convolution(pool1, @conv2.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 16, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu2, expr: "conv2 = ReLU(conv2)" }
  - { id: pool2, expr: "pool2 = Pooling(conv2)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool2relu, expr: "pool2 = ReLU(pool2)" }
  - { id: fc1, expr: "fc1 = FullyConnected(pool2, @fc1.w, @fc1.b)",
      attrs: {slice_axis: -2, num_output: 120} }
  - { id: fc1relu, expr: "fc1 = ReLU(fc1)" }
  - { id: fc2, expr: "fc2 = FullyConnected(fc1, @fc2.w, @fc2.b)",
      attrs: {slice_axis: -2, num_output: 84} }
  - { id: fc2relu, expr: "fc2 = ReLU(fc2)" }
  - { id: fc, expr: "fc = FullyConnected(fc2, @fc.w, @fc.b)",
      attrs: {slice_axis: -2, num_output: 10} }
  - { id: loss, expr: "loss = SoftmaxWithLoss(fc, label)",
      attrs: {axis: 0} }
  - { id: accuracy_top1, expr: "accuracy_top1 = Accuracy(fc, label)",
      attrs: {top_k: 1, slice_axis: 0} }
flows:
  - { name: main, inputs: [data, label], outputs: [loss, accuracy_top1], losses: [loss * 1, accuracy_top1 * 1] }

2017-03-21 13:30:25,946 INFO train: iter: 250, lr: 0.001000, ave loss: 2.04618285728
2017-03-21 13:30:25,946 INFO 	loss: 2.046183
2017-03-21 13:30:25,946 INFO 	accuracy_top1: 0.234250
2017-03-21 13:30:26,888 INFO train: iter: 500, lr: 0.001000, ave loss: 1.81478406298
2017-03-21 13:30:26,889 INFO 	loss: 1.814784
2017-03-21 13:30:26,889 INFO 	accuracy_top1: 0.326313
2017-03-21 13:30:27,031 INFO val: iter: 500, lr: 0.001000, ave loss: 1.80251023769
2017-03-21 13:30:27,031 INFO 	loss: 1.802510
2017-03-21 13:30:27,031 INFO 	accuracy_top1: 0.330000
2017-03-21 13:30:27,838 INFO train: iter: 750, lr: 0.001000, ave loss: 1.78938947022
2017-03-21 13:30:27,839 INFO 	loss: 1.789389
2017-03-21 13:30:27,839 INFO 	accuracy_top1: 0.337844
2017-03-21 13:30:28,674 INFO train: iter: 1000, lr: 0.001000, ave loss: 1.74223834652
2017-03-21 13:30:28,675 INFO 	loss: 1.742238
2017-03-21 13:30:28,675 INFO 	accuracy_top1: 0.356563
2017-03-21 13:30:28,804 INFO val: iter: 1000, lr: 0.001000, ave loss: 1.73196260482
2017-03-21 13:30:28,805 INFO 	loss: 1.731963
2017-03-21 13:30:28,805 INFO 	accuracy_top1: 0.356000
2017-03-21 13:30:29,707 INFO train: iter: 1250, lr: 0.001000, ave loss: 1.70859273291
2017-03-21 13:30:29,708 INFO 	loss: 1.708593
2017-03-21 13:30:29,708 INFO 	accuracy_top1: 0.375797
2017-03-21 13:30:30,556 INFO train: iter: 1500, lr: 0.001000, ave loss: 1.67813661742
2017-03-21 13:30:30,556 INFO 	loss: 1.678137
2017-03-21 13:30:30,557 INFO 	accuracy_top1: 0.386547
2017-03-21 13:30:30,735 INFO val: iter: 1500, lr: 0.001000, ave loss: 1.69212964028
2017-03-21 13:30:30,735 INFO 	loss: 1.692130
2017-03-21 13:30:30,735 INFO 	accuracy_top1: 0.378600
2017-03-21 13:30:31,640 INFO train: iter: 1750, lr: 0.001000, ave loss: 1.64442138463
2017-03-21 13:30:31,640 INFO 	loss: 1.644421
2017-03-21 13:30:31,641 INFO 	accuracy_top1: 0.400000
2017-03-21 13:30:32,581 INFO train: iter: 2000, lr: 0.001000, ave loss: 1.6232027567
2017-03-21 13:30:32,581 INFO 	loss: 1.623203
2017-03-21 13:30:32,581 INFO 	accuracy_top1: 0.408719
2017-03-21 13:30:32,727 INFO val: iter: 2000, lr: 0.001000, ave loss: 1.67851105779
2017-03-21 13:30:32,728 INFO 	loss: 1.678511
2017-03-21 13:30:32,728 INFO 	accuracy_top1: 0.391700
2017-03-21 13:30:33,730 INFO train: iter: 2250, lr: 0.001000, ave loss: 1.58751203021
2017-03-21 13:30:33,730 INFO 	loss: 1.587512
2017-03-21 13:30:33,731 INFO 	accuracy_top1: 0.428531
2017-03-21 13:30:34,633 INFO train: iter: 2500, lr: 0.001000, ave loss: 1.53615885037
2017-03-21 13:30:34,633 INFO 	loss: 1.536159
2017-03-21 13:30:34,633 INFO 	accuracy_top1: 0.441953
2017-03-21 13:30:34,761 INFO val: iter: 2500, lr: 0.001000, ave loss: 1.56413556635
2017-03-21 13:30:34,761 INFO 	loss: 1.564136
2017-03-21 13:30:34,761 INFO 	accuracy_top1: 0.436600
2017-03-21 13:30:35,712 INFO train: iter: 2750, lr: 0.001000, ave loss: 1.50190936241
2017-03-21 13:30:35,713 INFO 	loss: 1.501909
2017-03-21 13:30:35,713 INFO 	accuracy_top1: 0.459031
2017-03-21 13:30:36,558 INFO train: iter: 3000, lr: 0.001000, ave loss: 1.46483056885
2017-03-21 13:30:36,558 INFO 	loss: 1.464831
2017-03-21 13:30:36,558 INFO 	accuracy_top1: 0.471625
2017-03-21 13:30:36,689 INFO val: iter: 3000, lr: 0.001000, ave loss: 1.52831116617
2017-03-21 13:30:36,689 INFO 	loss: 1.528311
2017-03-21 13:30:36,690 INFO 	accuracy_top1: 0.461800
2017-03-21 13:30:37,551 INFO train: iter: 3250, lr: 0.001000, ave loss: 1.4424408634
2017-03-21 13:30:37,551 INFO 	loss: 1.442441
2017-03-21 13:30:37,551 INFO 	accuracy_top1: 0.483938
2017-03-21 13:30:38,513 INFO train: iter: 3500, lr: 0.001000, ave loss: 1.42728218171
2017-03-21 13:30:38,514 INFO 	loss: 1.427282
2017-03-21 13:30:38,514 INFO 	accuracy_top1: 0.486281
2017-03-21 13:30:38,710 INFO val: iter: 3500, lr: 0.001000, ave loss: 1.46825678349
2017-03-21 13:30:38,711 INFO 	loss: 1.468257
2017-03-21 13:30:38,711 INFO 	accuracy_top1: 0.474900
2017-03-21 13:30:39,764 INFO train: iter: 3750, lr: 0.001000, ave loss: 1.41250716361
2017-03-21 13:30:39,765 INFO 	loss: 1.412507
2017-03-21 13:30:39,765 INFO 	accuracy_top1: 0.496312
2017-03-21 13:30:40,803 INFO train: iter: 4000, lr: 0.001000, ave loss: 1.38676257661
2017-03-21 13:30:40,803 INFO 	loss: 1.386763
2017-03-21 13:30:40,803 INFO 	accuracy_top1: 0.503891
2017-03-21 13:30:40,968 INFO val: iter: 4000, lr: 0.001000, ave loss: 1.4289679125
2017-03-21 13:30:40,969 INFO 	loss: 1.428968
2017-03-21 13:30:40,969 INFO 	accuracy_top1: 0.493100
2017-03-21 13:30:41,904 INFO train: iter: 4250, lr: 0.001000, ave loss: 1.36164183602
2017-03-21 13:30:41,904 INFO 	loss: 1.361642
2017-03-21 13:30:41,905 INFO 	accuracy_top1: 0.515094
2017-03-21 13:30:42,843 INFO train: iter: 4500, lr: 0.001000, ave loss: 1.33093127447
2017-03-21 13:30:42,843 INFO 	loss: 1.330931
2017-03-21 13:30:42,843 INFO 	accuracy_top1: 0.527641
2017-03-21 13:30:42,974 INFO val: iter: 4500, lr: 0.001000, ave loss: 1.3872551024
2017-03-21 13:30:42,975 INFO 	loss: 1.387255
2017-03-21 13:30:42,975 INFO 	accuracy_top1: 0.506500
2017-03-21 13:30:43,803 INFO train: iter: 4750, lr: 0.001000, ave loss: 1.31913370904
2017-03-21 13:30:43,803 INFO 	loss: 1.319134
2017-03-21 13:30:43,803 INFO 	accuracy_top1: 0.531094
2017-03-21 13:30:44,670 INFO train: iter: 5000, lr: 0.001000, ave loss: 1.30923608831
2017-03-21 13:30:44,670 INFO 	loss: 1.309236
2017-03-21 13:30:44,670 INFO 	accuracy_top1: 0.537031
2017-03-21 13:30:44,807 INFO val: iter: 5000, lr: 0.001000, ave loss: 1.35167251825
2017-03-21 13:30:44,808 INFO 	loss: 1.351673
2017-03-21 13:30:44,808 INFO 	accuracy_top1: 0.524200
2017-03-21 13:30:45,691 INFO train: iter: 5250, lr: 0.001000, ave loss: 1.27794669524
2017-03-21 13:30:45,692 INFO 	loss: 1.277947
2017-03-21 13:30:45,692 INFO 	accuracy_top1: 0.549469
2017-03-21 13:30:46,558 INFO train: iter: 5500, lr: 0.001000, ave loss: 1.27072673708
2017-03-21 13:30:46,558 INFO 	loss: 1.270727
2017-03-21 13:30:46,559 INFO 	accuracy_top1: 0.550656
2017-03-21 13:30:46,685 INFO val: iter: 5500, lr: 0.001000, ave loss: 1.34932120293
2017-03-21 13:30:46,686 INFO 	loss: 1.349321
2017-03-21 13:30:46,686 INFO 	accuracy_top1: 0.533000
2017-03-21 13:30:47,574 INFO train: iter: 5750, lr: 0.001000, ave loss: 1.2593875891
2017-03-21 13:30:47,574 INFO 	loss: 1.259388
2017-03-21 13:30:47,575 INFO 	accuracy_top1: 0.557156
2017-03-21 13:30:48,371 INFO train: iter: 6000, lr: 0.001000, ave loss: 1.2366196683
2017-03-21 13:30:48,371 INFO 	loss: 1.236620
2017-03-21 13:30:48,371 INFO 	accuracy_top1: 0.563578
2017-03-21 13:30:48,514 INFO val: iter: 6000, lr: 0.001000, ave loss: 1.31629389375
2017-03-21 13:30:48,514 INFO 	loss: 1.316294
2017-03-21 13:30:48,515 INFO 	accuracy_top1: 0.535300
2017-03-21 13:30:49,427 INFO train: iter: 6250, lr: 0.001000, ave loss: 1.23829159132
2017-03-21 13:30:49,428 INFO 	loss: 1.238292
2017-03-21 13:30:49,428 INFO 	accuracy_top1: 0.563281
2017-03-21 13:30:50,316 INFO train: iter: 6500, lr: 0.001000, ave loss: 1.21216052246
2017-03-21 13:30:50,317 INFO 	loss: 1.212161
2017-03-21 13:30:50,317 INFO 	accuracy_top1: 0.574906
2017-03-21 13:30:50,465 INFO val: iter: 6500, lr: 0.001000, ave loss: 1.29399235845
2017-03-21 13:30:50,465 INFO 	loss: 1.293992
2017-03-21 13:30:50,466 INFO 	accuracy_top1: 0.544600
2017-03-21 13:30:51,392 INFO train: iter: 6750, lr: 0.001000, ave loss: 1.19660588783
2017-03-21 13:30:51,392 INFO 	loss: 1.196606
2017-03-21 13:30:51,392 INFO 	accuracy_top1: 0.577344
2017-03-21 13:30:52,236 INFO train: iter: 7000, lr: 0.001000, ave loss: 1.19845363939
2017-03-21 13:30:52,236 INFO 	loss: 1.198454
2017-03-21 13:30:52,236 INFO 	accuracy_top1: 0.580625
2017-03-21 13:30:52,373 INFO val: iter: 7000, lr: 0.001000, ave loss: 1.25633675903
2017-03-21 13:30:52,373 INFO 	loss: 1.256337
2017-03-21 13:30:52,373 INFO 	accuracy_top1: 0.563500
2017-03-21 13:30:53,223 INFO train: iter: 7250, lr: 0.001000, ave loss: 1.186996966
2017-03-21 13:30:53,223 INFO 	loss: 1.186997
2017-03-21 13:30:53,223 INFO 	accuracy_top1: 0.583859
2017-03-21 13:30:54,204 INFO train: iter: 7500, lr: 0.001000, ave loss: 1.16743633115
2017-03-21 13:30:54,204 INFO 	loss: 1.167436
2017-03-21 13:30:54,204 INFO 	accuracy_top1: 0.591484
2017-03-21 13:30:54,325 INFO val: iter: 7500, lr: 0.001000, ave loss: 1.24907736927
2017-03-21 13:30:54,325 INFO 	loss: 1.249077
2017-03-21 13:30:54,325 INFO 	accuracy_top1: 0.563500
2017-03-21 13:30:55,186 INFO train: iter: 7750, lr: 0.001000, ave loss: 1.15579034029
2017-03-21 13:30:55,187 INFO 	loss: 1.155790
2017-03-21 13:30:55,187 INFO 	accuracy_top1: 0.592297
2017-03-21 13:30:56,073 INFO train: iter: 8000, lr: 0.000100, ave loss: 1.15014110181
2017-03-21 13:30:56,074 INFO 	loss: 1.150141
2017-03-21 13:30:56,074 INFO 	accuracy_top1: 0.592234
2017-03-21 13:30:56,198 INFO val: iter: 8000, lr: 0.000100, ave loss: 1.261576204
2017-03-21 13:30:56,198 INFO 	loss: 1.261576
2017-03-21 13:30:56,198 INFO 	accuracy_top1: 0.559300
2017-03-21 13:30:57,143 INFO train: iter: 8250, lr: 0.000100, ave loss: 1.09008213341
2017-03-21 13:30:57,143 INFO 	loss: 1.090082
2017-03-21 13:30:57,143 INFO 	accuracy_top1: 0.617859
2017-03-21 13:30:58,162 INFO train: iter: 8500, lr: 0.000100, ave loss: 1.06205738658
2017-03-21 13:30:58,162 INFO 	loss: 1.062057
2017-03-21 13:30:58,163 INFO 	accuracy_top1: 0.628188
2017-03-21 13:30:58,317 INFO val: iter: 8500, lr: 0.000100, ave loss: 1.1956389308
2017-03-21 13:30:58,318 INFO 	loss: 1.195639
2017-03-21 13:30:58,318 INFO 	accuracy_top1: 0.583900
2017-03-21 13:30:59,357 INFO train: iter: 8750, lr: 0.000100, ave loss: 1.06272156322
2017-03-21 13:30:59,357 INFO 	loss: 1.062722
2017-03-21 13:30:59,357 INFO 	accuracy_top1: 0.628844
2017-03-21 13:31:00,345 INFO train: iter: 9000, lr: 0.000100, ave loss: 1.06444265988
2017-03-21 13:31:00,345 INFO 	loss: 1.064443
2017-03-21 13:31:00,345 INFO 	accuracy_top1: 0.627156
2017-03-21 13:31:00,503 INFO val: iter: 9000, lr: 0.000100, ave loss: 1.18731948435
2017-03-21 13:31:00,503 INFO 	loss: 1.187319
2017-03-21 13:31:00,503 INFO 	accuracy_top1: 0.586300
2017-03-21 13:31:01,490 INFO train: iter: 9250, lr: 0.000100, ave loss: 1.05414837804
2017-03-21 13:31:01,490 INFO 	loss: 1.054148
2017-03-21 13:31:01,490 INFO 	accuracy_top1: 0.628297
2017-03-21 13:31:02,347 INFO train: iter: 9500, lr: 0.000100, ave loss: 1.06108283496
2017-03-21 13:31:02,347 INFO 	loss: 1.061083
2017-03-21 13:31:02,347 INFO 	accuracy_top1: 0.628781
2017-03-21 13:31:02,470 INFO val: iter: 9500, lr: 0.000100, ave loss: 1.19148481935
2017-03-21 13:31:02,470 INFO 	loss: 1.191485
2017-03-21 13:31:02,470 INFO 	accuracy_top1: 0.586500
2017-03-21 13:31:03,331 INFO train: iter: 9750, lr: 0.000100, ave loss: 1.05746546072
2017-03-21 13:31:03,331 INFO 	loss: 1.057465
2017-03-21 13:31:03,331 INFO 	accuracy_top1: 0.628938
2017-03-21 13:31:04,206 INFO train: iter: 10000, lr: 0.000010, ave loss: 1.04796789557
2017-03-21 13:31:04,206 INFO 	loss: 1.047968
2017-03-21 13:31:04,206 INFO 	accuracy_top1: 0.632766
2017-03-21 13:31:04,344 INFO val: iter: 10000, lr: 0.000010, ave loss: 1.18851708621
2017-03-21 13:31:04,344 INFO 	loss: 1.188517
2017-03-21 13:31:04,344 INFO 	accuracy_top1: 0.585000
2017-03-21 13:31:05,273 INFO train: iter: 10250, lr: 0.000010, ave loss: 1.03863154872
2017-03-21 13:31:05,274 INFO 	loss: 1.038632
2017-03-21 13:31:05,274 INFO 	accuracy_top1: 0.634234
2017-03-21 13:31:06,257 INFO train: iter: 10500, lr: 0.000010, ave loss: 1.05054704067
2017-03-21 13:31:06,257 INFO 	loss: 1.050547
2017-03-21 13:31:06,257 INFO 	accuracy_top1: 0.632219
2017-03-21 13:31:06,420 INFO val: iter: 10500, lr: 0.000010, ave loss: 1.17957984656
2017-03-21 13:31:06,420 INFO 	loss: 1.179580
2017-03-21 13:31:06,421 INFO 	accuracy_top1: 0.588600
2017-03-21 13:31:07,387 INFO train: iter: 10750, lr: 0.000010, ave loss: 1.04900770473
2017-03-21 13:31:07,387 INFO 	loss: 1.049008
2017-03-21 13:31:07,388 INFO 	accuracy_top1: 0.632234
2017-03-21 13:31:08,270 INFO train: iter: 11000, lr: 0.000010, ave loss: 1.04234036329
2017-03-21 13:31:08,271 INFO 	loss: 1.042340
2017-03-21 13:31:08,271 INFO 	accuracy_top1: 0.634000
2017-03-21 13:31:08,406 INFO val: iter: 11000, lr: 0.000010, ave loss: 1.17924133763
2017-03-21 13:31:08,406 INFO 	loss: 1.179241
2017-03-21 13:31:08,406 INFO 	accuracy_top1: 0.589100
2017-03-21 13:31:09,275 INFO train: iter: 11250, lr: 0.000010, ave loss: 1.04176109344
2017-03-21 13:31:09,275 INFO 	loss: 1.041761
2017-03-21 13:31:09,275 INFO 	accuracy_top1: 0.636625
2017-03-21 13:31:10,141 INFO train: iter: 11500, lr: 0.000010, ave loss: 1.04328721675
2017-03-21 13:31:10,141 INFO 	loss: 1.043287
2017-03-21 13:31:10,141 INFO 	accuracy_top1: 0.632203
2017-03-21 13:31:10,267 INFO val: iter: 11500, lr: 0.000010, ave loss: 1.17969996333
2017-03-21 13:31:10,267 INFO 	loss: 1.179700
2017-03-21 13:31:10,268 INFO 	accuracy_top1: 0.587900
2017-03-21 13:31:11,163 INFO train: iter: 11750, lr: 0.000010, ave loss: 1.04614033699
2017-03-21 13:31:11,163 INFO 	loss: 1.046140
2017-03-21 13:31:11,163 INFO 	accuracy_top1: 0.634266
2017-03-21 13:31:12,190 INFO train: iter: 12000, lr: 0.000010, ave loss: 1.04222909981
2017-03-21 13:31:12,190 INFO 	loss: 1.042229
2017-03-21 13:31:12,190 INFO 	accuracy_top1: 0.633906
2017-03-21 13:31:12,338 INFO val: iter: 12000, lr: 0.000010, ave loss: 1.17975558788
2017-03-21 13:31:12,339 INFO 	loss: 1.179756
2017-03-21 13:31:12,339 INFO 	accuracy_top1: 0.589200
2017-03-21 13:32:52,496 INFO connecting pavi service http://pavi.parrotsdnn.org/log
2017-03-21 13:32:52,530 INFO pavi service connected, instance_id: 72d8cfd67fd34c608032840383714768
2017-03-21 13:32:52,552 INFO model name: net1_msra
2017-03-21 13:32:52,553 DEBUG name: "net1_msra"
inputs:
  - { id: data , spec: "Float32(32, 32, 3, _)" }
  - { id: label, spec: "Uint32(1, _)" }
params:
  - { id: conv1.w, spec: "Float32(5, 5, 3, 6)" , learning-policy: { init: xavier, lr_mult: 1, decay_mult: 1 } }
  - { id: conv2.w, spec: "Float32(5, 5, 6, 16)", learning-policy: { init: xavier, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.w  , spec: "Float32(400, 120)"   , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.b  , spec: "Float32(120, 1)"     , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc2.w  , spec: "Float32(120, 84)"    , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc2.b  , spec: "Float32(84, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc.w   , spec: "Float32(84, 10)"     , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc.b   , spec: "Float32(10, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
layers:
  - { id: conv1, expr: "conv1 = Convolution(data, @conv1.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 6, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu1, expr: "conv1 = ReLU(conv1)" }
  - { id: pool1, expr: "pool1 = Pooling(conv1)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool1relu, expr: "pool1 = ReLU(pool1)" }
  - { id: conv2, expr: "conv2 = Convolution(pool1, @conv2.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 16, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu2, expr: "conv2 = ReLU(conv2)" }
  - { id: pool2, expr: "pool2 = Pooling(conv2)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool2relu, expr: "pool2 = ReLU(pool2)" }
  - { id: fc1, expr: "fc1 = FullyConnected(pool2, @fc1.w, @fc1.b)",
      attrs: {slice_axis: -2, num_output: 120} }
  - { id: fc1relu, expr: "fc1 = ReLU(fc1)" }
  - { id: fc2, expr: "fc2 = FullyConnected(fc1, @fc2.w, @fc2.b)",
      attrs: {slice_axis: -2, num_output: 84} }
  - { id: fc2relu, expr: "fc2 = ReLU(fc2)" }
  - { id: fc, expr: "fc = FullyConnected(fc2, @fc.w, @fc.b)",
      attrs: {slice_axis: -2, num_output: 10} }
  - { id: loss, expr: "loss = SoftmaxWithLoss(fc, label)",
      attrs: {axis: 0} }
  - { id: accuracy_top1, expr: "accuracy_top1 = Accuracy(fc, label)",
      attrs: {top_k: 1, slice_axis: 0} }
flows:
  - { name: main, inputs: [data, label], outputs: [loss, accuracy_top1], losses: [loss * 1, accuracy_top1 * 1] }

2017-03-21 13:32:54,335 INFO train: iter: 250, lr: 0.001000, ave loss: 2.73375611103
2017-03-21 13:32:54,335 INFO 	loss: 2.733756
2017-03-21 13:32:54,335 INFO 	accuracy_top1: 0.099672
2017-03-21 13:32:55,355 INFO train: iter: 500, lr: 0.001000, ave loss: 2.30260924721
2017-03-21 13:32:55,355 INFO 	loss: 2.302609
2017-03-21 13:32:55,355 INFO 	accuracy_top1: 0.097641
2017-03-21 13:32:55,550 INFO val: iter: 500, lr: 0.001000, ave loss: 2.30259631276
2017-03-21 13:32:55,551 INFO 	loss: 2.302596
2017-03-21 13:32:55,551 INFO 	accuracy_top1: 0.100000
2017-03-21 13:32:56,528 INFO train: iter: 750, lr: 0.001000, ave loss: 2.30258932996
2017-03-21 13:32:56,528 INFO 	loss: 2.302589
2017-03-21 13:32:56,529 INFO 	accuracy_top1: 0.101000
2017-03-21 13:32:57,444 INFO train: iter: 1000, lr: 0.001000, ave loss: 2.30261703491
2017-03-21 13:32:57,444 INFO 	loss: 2.302617
2017-03-21 13:32:57,445 INFO 	accuracy_top1: 0.098281
2017-03-21 13:32:57,571 INFO val: iter: 1000, lr: 0.001000, ave loss: 2.30259058475
2017-03-21 13:32:57,571 INFO 	loss: 2.302591
2017-03-21 13:32:57,571 INFO 	accuracy_top1: 0.100000
2017-03-21 13:32:58,430 INFO train: iter: 1250, lr: 0.001000, ave loss: 2.30259752846
2017-03-21 13:32:58,431 INFO 	loss: 2.302598
2017-03-21 13:32:58,431 INFO 	accuracy_top1: 0.099062
2017-03-21 13:32:59,362 INFO train: iter: 1500, lr: 0.001000, ave loss: 2.30259849107
2017-03-21 13:32:59,363 INFO 	loss: 2.302598
2017-03-21 13:32:59,363 INFO 	accuracy_top1: 0.100187
2017-03-21 13:32:59,555 INFO val: iter: 1500, lr: 0.001000, ave loss: 2.30259473026
2017-03-21 13:32:59,556 INFO 	loss: 2.302595
2017-03-21 13:32:59,556 INFO 	accuracy_top1: 0.100000
2017-03-21 13:33:00,584 INFO train: iter: 1750, lr: 0.001000, ave loss: 2.30262656724
2017-03-21 13:33:00,584 INFO 	loss: 2.302627
2017-03-21 13:33:00,584 INFO 	accuracy_top1: 0.098859
2017-03-21 13:33:01,551 INFO train: iter: 2000, lr: 0.001000, ave loss: 2.30260082841
2017-03-21 13:33:01,551 INFO 	loss: 2.302601
2017-03-21 13:33:01,551 INFO 	accuracy_top1: 0.099484
2017-03-21 13:33:01,686 INFO val: iter: 2000, lr: 0.001000, ave loss: 2.30259362757
2017-03-21 13:33:01,686 INFO 	loss: 2.302594
2017-03-21 13:33:01,686 INFO 	accuracy_top1: 0.100000
2017-03-21 13:33:02,782 INFO train: iter: 2250, lr: 0.001000, ave loss: 2.30257644725
2017-03-21 13:33:02,783 INFO 	loss: 2.302576
2017-03-21 13:33:02,783 INFO 	accuracy_top1: 0.099641
2017-03-21 13:33:03,922 INFO train: iter: 2500, lr: 0.001000, ave loss: 2.30257641029
2017-03-21 13:33:03,922 INFO 	loss: 2.302576
2017-03-21 13:33:03,922 INFO 	accuracy_top1: 0.098984
2017-03-21 13:33:04,077 INFO val: iter: 2500, lr: 0.001000, ave loss: 2.30260060728
2017-03-21 13:33:04,077 INFO 	loss: 2.302601
2017-03-21 13:33:04,078 INFO 	accuracy_top1: 0.100000
2017-03-21 13:33:05,128 INFO train: iter: 2750, lr: 0.001000, ave loss: 2.30262292302
2017-03-21 13:33:05,128 INFO 	loss: 2.302623
2017-03-21 13:33:05,128 INFO 	accuracy_top1: 0.098672
2017-03-21 13:33:06,139 INFO train: iter: 3000, lr: 0.001000, ave loss: 2.30261888731
2017-03-21 13:33:06,140 INFO 	loss: 2.302619
2017-03-21 13:33:06,140 INFO 	accuracy_top1: 0.099437
2017-03-21 13:33:06,297 INFO val: iter: 3000, lr: 0.001000, ave loss: 2.3025886327
2017-03-21 13:33:06,297 INFO 	loss: 2.302589
2017-03-21 13:33:06,297 INFO 	accuracy_top1: 0.100000
2017-03-21 13:33:07,192 INFO train: iter: 3250, lr: 0.001000, ave loss: 2.30259444129
2017-03-21 13:33:07,193 INFO 	loss: 2.302594
2017-03-21 13:33:07,193 INFO 	accuracy_top1: 0.098984
2017-03-21 13:33:08,151 INFO train: iter: 3500, lr: 0.001000, ave loss: 2.3025924536
2017-03-21 13:33:08,152 INFO 	loss: 2.302592
2017-03-21 13:33:08,152 INFO 	accuracy_top1: 0.099797
2017-03-21 13:33:08,312 INFO val: iter: 3500, lr: 0.001000, ave loss: 2.30259318948
2017-03-21 13:33:08,312 INFO 	loss: 2.302593
2017-03-21 13:33:08,313 INFO 	accuracy_top1: 0.100000
2017-03-21 13:33:09,214 INFO train: iter: 3750, lr: 0.001000, ave loss: 2.30259706771
2017-03-21 13:33:09,214 INFO 	loss: 2.302597
2017-03-21 13:33:09,214 INFO 	accuracy_top1: 0.099391
2017-03-21 13:33:10,169 INFO train: iter: 4000, lr: 0.001000, ave loss: 2.3025487206
2017-03-21 13:33:10,170 INFO 	loss: 2.302549
2017-03-21 13:33:10,170 INFO 	accuracy_top1: 0.100703
2017-03-21 13:33:10,306 INFO val: iter: 4000, lr: 0.001000, ave loss: 2.30260369182
2017-03-21 13:33:10,307 INFO 	loss: 2.302604
2017-03-21 13:33:10,307 INFO 	accuracy_top1: 0.100000
2017-03-21 13:33:11,214 INFO train: iter: 4250, lr: 0.001000, ave loss: 2.3026236825
2017-03-21 13:33:11,215 INFO 	loss: 2.302624
2017-03-21 13:33:11,215 INFO 	accuracy_top1: 0.100250
2017-03-21 13:33:12,076 INFO train: iter: 4500, lr: 0.001000, ave loss: 2.30260777342
2017-03-21 13:33:12,076 INFO 	loss: 2.302608
2017-03-21 13:33:12,076 INFO 	accuracy_top1: 0.099625
2017-03-21 13:33:12,201 INFO val: iter: 4500, lr: 0.001000, ave loss: 2.30258829594
2017-03-21 13:33:12,201 INFO 	loss: 2.302588
2017-03-21 13:33:12,202 INFO 	accuracy_top1: 0.100000
2017-03-21 13:33:12,642 ERROR KeyboardInterrupt
Traceback (most recent call last):
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/runner.py", line 177, in run
    for logs in policy.run():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/basepolicy.py", line 19, in run
    for logs in self.iter():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/iterate.py", line 47, in iter
    self.flow.iterate()
KeyboardInterrupt
2017-03-21 13:41:37,196 INFO connecting pavi service http://pavi.parrotsdnn.org/log
2017-03-21 13:41:37,277 INFO pavi service connected, instance_id: 90401eda796c4465ad21e31eba43f911
2017-03-21 13:41:37,301 INFO model name: net1_msra
2017-03-21 13:41:37,302 DEBUG name: "net1_msra"
inputs:
  - { id: data , spec: "Float32(32, 32, 3, _)" }
  - { id: label, spec: "Uint32(1, _)" }
params:
  - { id: conv1.w, spec: "Float32(5, 5, 3, 6)" , learning-policy: { init: gauss(0.01), lr_mult: 1, decay_mult: 1 } }
  - { id: conv2.w, spec: "Float32(5, 5, 6, 16)", learning-policy: { init: gauss(0.01), lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.w  , spec: "Float32(400, 120)"   , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.b  , spec: "Float32(120, 1)"     , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc2.w  , spec: "Float32(120, 84)"    , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc2.b  , spec: "Float32(84, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc.w   , spec: "Float32(84, 10)"     , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc.b   , spec: "Float32(10, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
layers:
  - { id: conv1, expr: "conv1 = Convolution(data, @conv1.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 6, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu1, expr: "conv1 = ReLU(conv1)" }
  - { id: pool1, expr: "pool1 = Pooling(conv1)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool1relu, expr: "pool1 = ReLU(pool1)" }
  - { id: conv2, expr: "conv2 = Convolution(pool1, @conv2.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 16, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu2, expr: "conv2 = ReLU(conv2)" }
  - { id: pool2, expr: "pool2 = Pooling(conv2)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool2relu, expr: "pool2 = ReLU(pool2)" }
  - { id: fc1, expr: "fc1 = FullyConnected(pool2, @fc1.w, @fc1.b)",
      attrs: {slice_axis: -2, num_output: 120} }
  - { id: fc1relu, expr: "fc1 = ReLU(fc1)" }
  - { id: fc2, expr: "fc2 = FullyConnected(fc1, @fc2.w, @fc2.b)",
      attrs: {slice_axis: -2, num_output: 84} }
  - { id: fc2relu, expr: "fc2 = ReLU(fc2)" }
  - { id: fc, expr: "fc = FullyConnected(fc2, @fc.w, @fc.b)",
      attrs: {slice_axis: -2, num_output: 10} }
  - { id: loss, expr: "loss = SoftmaxWithLoss(fc, label)",
      attrs: {axis: 0} }
  - { id: accuracy_top1, expr: "accuracy_top1 = Accuracy(fc, label)",
      attrs: {top_k: 1, slice_axis: 0} }
flows:
  - { name: main, inputs: [data, label], outputs: [loss, accuracy_top1], losses: [loss * 1, accuracy_top1 * 1] }

2017-03-21 13:41:39,018 INFO train: iter: 250, lr: 0.001000, ave loss: 2.29352685547
2017-03-21 13:41:39,018 INFO 	loss: 2.293527
2017-03-21 13:41:39,018 INFO 	accuracy_top1: 0.145078
2017-03-21 13:41:39,973 INFO train: iter: 500, lr: 0.001000, ave loss: 2.20355785102
2017-03-21 13:41:39,973 INFO 	loss: 2.203558
2017-03-21 13:41:39,973 INFO 	accuracy_top1: 0.208203
2017-03-21 13:41:40,128 INFO val: iter: 500, lr: 0.001000, ave loss: 2.06243774742
2017-03-21 13:41:40,128 INFO 	loss: 2.062438
2017-03-21 13:41:40,128 INFO 	accuracy_top1: 0.267200
2017-03-21 13:41:41,001 INFO train: iter: 750, lr: 0.001000, ave loss: 1.94232929236
2017-03-21 13:41:41,001 INFO 	loss: 1.942329
2017-03-21 13:41:41,001 INFO 	accuracy_top1: 0.296141
2017-03-21 13:41:41,863 INFO train: iter: 1000, lr: 0.001000, ave loss: 1.77593385506
2017-03-21 13:41:41,863 INFO 	loss: 1.775934
2017-03-21 13:41:41,864 INFO 	accuracy_top1: 0.352563
2017-03-21 13:41:41,977 INFO val: iter: 1000, lr: 0.001000, ave loss: 1.70317527205
2017-03-21 13:41:41,977 INFO 	loss: 1.703175
2017-03-21 13:41:41,977 INFO 	accuracy_top1: 0.376400
2017-03-21 13:41:42,786 INFO train: iter: 1250, lr: 0.001000, ave loss: 1.66585091811
2017-03-21 13:41:42,786 INFO 	loss: 1.665851
2017-03-21 13:41:42,786 INFO 	accuracy_top1: 0.392594
2017-03-21 13:41:43,654 INFO train: iter: 1500, lr: 0.001000, ave loss: 1.59522989905
2017-03-21 13:41:43,654 INFO 	loss: 1.595230
2017-03-21 13:41:43,655 INFO 	accuracy_top1: 0.419281
2017-03-21 13:41:43,828 INFO val: iter: 1500, lr: 0.001000, ave loss: 1.55907178372
2017-03-21 13:41:43,828 INFO 	loss: 1.559072
2017-03-21 13:41:43,828 INFO 	accuracy_top1: 0.430100
2017-03-21 13:41:44,794 INFO train: iter: 1750, lr: 0.001000, ave loss: 1.53796470374
2017-03-21 13:41:44,794 INFO 	loss: 1.537965
2017-03-21 13:41:44,795 INFO 	accuracy_top1: 0.443328
2017-03-21 13:41:45,390 ERROR KeyboardInterrupt
Traceback (most recent call last):
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/runner.py", line 177, in run
    for logs in policy.run():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/basepolicy.py", line 19, in run
    for logs in self.iter():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/iterate.py", line 47, in iter
    self.flow.iterate()
KeyboardInterrupt
2017-03-21 13:41:58,676 INFO connecting pavi service http://pavi.parrotsdnn.org/log
2017-03-21 13:41:58,709 INFO pavi service connected, instance_id: b0fbf3308b5648d2b0077d4c48b4a580
2017-03-21 13:41:58,725 INFO model name: net1_msra
2017-03-21 13:41:58,725 DEBUG name: "net1_msra"
inputs:
  - { id: data , spec: "Float32(32, 32, 3, _)" }
  - { id: label, spec: "Uint32(1, _)" }
params:
  - { id: conv1.w, spec: "Float32(5, 5, 3, 6)" , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: conv2.w, spec: "Float32(5, 5, 6, 16)", learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.w  , spec: "Float32(400, 120)"   , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.b  , spec: "Float32(120, 1)"     , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc2.w  , spec: "Float32(120, 84)"    , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc2.b  , spec: "Float32(84, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc.w   , spec: "Float32(84, 10)"     , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc.b   , spec: "Float32(10, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
layers:
  - { id: conv1, expr: "conv1 = Convolution(data, @conv1.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 6, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu1, expr: "conv1 = ReLU(conv1)" }
  - { id: pool1, expr: "pool1 = Pooling(conv1)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool1relu, expr: "pool1 = ReLU(pool1)" }
  - { id: conv2, expr: "conv2 = Convolution(pool1, @conv2.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 16, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu2, expr: "conv2 = ReLU(conv2)" }
  - { id: pool2, expr: "pool2 = Pooling(conv2)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool2relu, expr: "pool2 = ReLU(pool2)" }
  - { id: fc1, expr: "fc1 = FullyConnected(pool2, @fc1.w, @fc1.b)",
      attrs: {slice_axis: -2, num_output: 120} }
  - { id: fc1relu, expr: "fc1 = ReLU(fc1)" }
  - { id: fc2, expr: "fc2 = FullyConnected(fc1, @fc2.w, @fc2.b)",
      attrs: {slice_axis: -2, num_output: 84} }
  - { id: fc2relu, expr: "fc2 = ReLU(fc2)" }
  - { id: fc, expr: "fc = FullyConnected(fc2, @fc.w, @fc.b)",
      attrs: {slice_axis: -2, num_output: 10} }
  - { id: loss, expr: "loss = SoftmaxWithLoss(fc, label)",
      attrs: {axis: 0} }
  - { id: accuracy_top1, expr: "accuracy_top1 = Accuracy(fc, label)",
      attrs: {top_k: 1, slice_axis: 0} }
flows:
  - { name: main, inputs: [data, label], outputs: [loss, accuracy_top1], losses: [loss * 1, accuracy_top1 * 1] }

2017-03-21 13:41:59,987 INFO train: iter: 250, lr: 0.001000, ave loss: 2.14800516385
2017-03-21 13:41:59,987 INFO 	loss: 2.148005
2017-03-21 13:41:59,987 INFO 	accuracy_top1: 0.221734
2017-03-21 13:42:00,937 INFO train: iter: 500, lr: 0.001000, ave loss: 1.8595540576
2017-03-21 13:42:00,937 INFO 	loss: 1.859554
2017-03-21 13:42:00,937 INFO 	accuracy_top1: 0.330984
2017-03-21 13:42:01,104 INFO val: iter: 500, lr: 0.001000, ave loss: 1.76563407183
2017-03-21 13:42:01,104 INFO 	loss: 1.765634
2017-03-21 13:42:01,104 INFO 	accuracy_top1: 0.359400
2017-03-21 13:42:01,928 INFO train: iter: 750, lr: 0.001000, ave loss: 1.69890583462
2017-03-21 13:42:01,928 INFO 	loss: 1.698906
2017-03-21 13:42:01,928 INFO 	accuracy_top1: 0.386391
2017-03-21 13:42:02,748 INFO train: iter: 1000, lr: 0.001000, ave loss: 1.61686231172
2017-03-21 13:42:02,748 INFO 	loss: 1.616862
2017-03-21 13:42:02,748 INFO 	accuracy_top1: 0.414641
2017-03-21 13:42:02,882 INFO val: iter: 1000, lr: 0.001000, ave loss: 1.5909891516
2017-03-21 13:42:02,882 INFO 	loss: 1.590989
2017-03-21 13:42:02,883 INFO 	accuracy_top1: 0.428000
2017-03-21 13:42:03,853 INFO train: iter: 1250, lr: 0.001000, ave loss: 1.57386751217
2017-03-21 13:42:03,853 INFO 	loss: 1.573868
2017-03-21 13:42:03,853 INFO 	accuracy_top1: 0.433750
2017-03-21 13:42:04,790 INFO train: iter: 1500, lr: 0.001000, ave loss: 1.51456960005
2017-03-21 13:42:04,790 INFO 	loss: 1.514570
2017-03-21 13:42:04,791 INFO 	accuracy_top1: 0.455828
2017-03-21 13:42:04,916 INFO val: iter: 1500, lr: 0.001000, ave loss: 1.51822681725
2017-03-21 13:42:04,916 INFO 	loss: 1.518227
2017-03-21 13:42:04,917 INFO 	accuracy_top1: 0.455300
2017-03-21 13:42:05,790 INFO train: iter: 1750, lr: 0.001000, ave loss: 1.48959509122
2017-03-21 13:42:05,790 INFO 	loss: 1.489595
2017-03-21 13:42:05,790 INFO 	accuracy_top1: 0.468109
2017-03-21 13:42:06,704 INFO train: iter: 2000, lr: 0.001000, ave loss: 1.45661463967
2017-03-21 13:42:06,704 INFO 	loss: 1.456615
2017-03-21 13:42:06,704 INFO 	accuracy_top1: 0.479797
2017-03-21 13:42:06,823 INFO val: iter: 2000, lr: 0.001000, ave loss: 1.46057087332
2017-03-21 13:42:06,823 INFO 	loss: 1.460571
2017-03-21 13:42:06,823 INFO 	accuracy_top1: 0.476700
2017-03-21 13:42:07,698 INFO train: iter: 2250, lr: 0.001000, ave loss: 1.42567816541
2017-03-21 13:42:07,698 INFO 	loss: 1.425678
2017-03-21 13:42:07,698 INFO 	accuracy_top1: 0.493641
2017-03-21 13:42:08,626 INFO train: iter: 2500, lr: 0.001000, ave loss: 1.40799148223
2017-03-21 13:42:08,627 INFO 	loss: 1.407991
2017-03-21 13:42:08,627 INFO 	accuracy_top1: 0.500313
2017-03-21 13:42:08,758 INFO val: iter: 2500, lr: 0.001000, ave loss: 1.41695029438
2017-03-21 13:42:08,759 INFO 	loss: 1.416950
2017-03-21 13:42:08,759 INFO 	accuracy_top1: 0.492700
2017-03-21 13:42:09,640 INFO train: iter: 2750, lr: 0.001000, ave loss: 1.37315875402
2017-03-21 13:42:09,641 INFO 	loss: 1.373159
2017-03-21 13:42:09,641 INFO 	accuracy_top1: 0.514281
2017-03-21 13:42:10,485 INFO train: iter: 3000, lr: 0.001000, ave loss: 1.34760885423
2017-03-21 13:42:10,485 INFO 	loss: 1.347609
2017-03-21 13:42:10,485 INFO 	accuracy_top1: 0.524547
2017-03-21 13:42:10,618 INFO val: iter: 3000, lr: 0.001000, ave loss: 1.36883520484
2017-03-21 13:42:10,619 INFO 	loss: 1.368835
2017-03-21 13:42:10,619 INFO 	accuracy_top1: 0.506500
2017-03-21 13:42:11,531 INFO train: iter: 3250, lr: 0.001000, ave loss: 1.33540831295
2017-03-21 13:42:11,531 INFO 	loss: 1.335408
2017-03-21 13:42:11,531 INFO 	accuracy_top1: 0.529047
2017-03-21 13:42:12,492 INFO train: iter: 3500, lr: 0.001000, ave loss: 1.30984231749
2017-03-21 13:42:12,492 INFO 	loss: 1.309842
2017-03-21 13:42:12,492 INFO 	accuracy_top1: 0.536344
2017-03-21 13:42:12,640 INFO val: iter: 3500, lr: 0.001000, ave loss: 1.34354632646
2017-03-21 13:42:12,641 INFO 	loss: 1.343546
2017-03-21 13:42:12,641 INFO 	accuracy_top1: 0.521500
2017-03-21 13:42:13,694 INFO train: iter: 3750, lr: 0.001000, ave loss: 1.28701634532
2017-03-21 13:42:13,694 INFO 	loss: 1.287016
2017-03-21 13:42:13,694 INFO 	accuracy_top1: 0.547344
2017-03-21 13:42:14,840 INFO train: iter: 4000, lr: 0.001000, ave loss: 1.27136902496
2017-03-21 13:42:14,840 INFO 	loss: 1.271369
2017-03-21 13:42:14,840 INFO 	accuracy_top1: 0.550188
2017-03-21 13:42:15,013 INFO val: iter: 4000, lr: 0.001000, ave loss: 1.30065843016
2017-03-21 13:42:15,013 INFO 	loss: 1.300658
2017-03-21 13:42:15,013 INFO 	accuracy_top1: 0.536900
2017-03-21 13:42:15,965 INFO train: iter: 4250, lr: 0.001000, ave loss: 1.24208075076
2017-03-21 13:42:15,965 INFO 	loss: 1.242081
2017-03-21 13:42:15,965 INFO 	accuracy_top1: 0.564906
2017-03-21 13:42:16,996 INFO train: iter: 4500, lr: 0.001000, ave loss: 1.23892041799
2017-03-21 13:42:16,996 INFO 	loss: 1.238920
2017-03-21 13:42:16,996 INFO 	accuracy_top1: 0.562953
2017-03-21 13:42:17,146 INFO val: iter: 4500, lr: 0.001000, ave loss: 1.27140522301
2017-03-21 13:42:17,147 INFO 	loss: 1.271405
2017-03-21 13:42:17,147 INFO 	accuracy_top1: 0.549300
2017-03-21 13:42:18,157 INFO train: iter: 4750, lr: 0.001000, ave loss: 1.21949181029
2017-03-21 13:42:18,158 INFO 	loss: 1.219492
2017-03-21 13:42:18,158 INFO 	accuracy_top1: 0.569203
2017-03-21 13:42:19,122 INFO train: iter: 5000, lr: 0.001000, ave loss: 1.21229434541
2017-03-21 13:42:19,122 INFO 	loss: 1.212294
2017-03-21 13:42:19,123 INFO 	accuracy_top1: 0.573391
2017-03-21 13:42:19,269 INFO val: iter: 5000, lr: 0.001000, ave loss: 1.25287558734
2017-03-21 13:42:19,270 INFO 	loss: 1.252876
2017-03-21 13:42:19,270 INFO 	accuracy_top1: 0.556400
2017-03-21 13:42:20,172 INFO train: iter: 5250, lr: 0.001000, ave loss: 1.19796841681
2017-03-21 13:42:20,173 INFO 	loss: 1.197968
2017-03-21 13:42:20,173 INFO 	accuracy_top1: 0.579016
2017-03-21 13:42:21,082 INFO train: iter: 5500, lr: 0.001000, ave loss: 1.18102297267
2017-03-21 13:42:21,083 INFO 	loss: 1.181023
2017-03-21 13:42:21,083 INFO 	accuracy_top1: 0.582344
2017-03-21 13:42:21,223 INFO val: iter: 5500, lr: 0.001000, ave loss: 1.23335772008
2017-03-21 13:42:21,223 INFO 	loss: 1.233358
2017-03-21 13:42:21,223 INFO 	accuracy_top1: 0.561700
2017-03-21 13:42:22,179 INFO train: iter: 5750, lr: 0.001000, ave loss: 1.17327031359
2017-03-21 13:42:22,180 INFO 	loss: 1.173270
2017-03-21 13:42:22,180 INFO 	accuracy_top1: 0.587219
2017-03-21 13:42:23,115 INFO train: iter: 6000, lr: 0.001000, ave loss: 1.15681507754
2017-03-21 13:42:23,115 INFO 	loss: 1.156815
2017-03-21 13:42:23,115 INFO 	accuracy_top1: 0.592297
2017-03-21 13:42:23,254 INFO val: iter: 6000, lr: 0.001000, ave loss: 1.2125162214
2017-03-21 13:42:23,254 INFO 	loss: 1.212516
2017-03-21 13:42:23,254 INFO 	accuracy_top1: 0.571400
2017-03-21 13:42:24,112 INFO train: iter: 6250, lr: 0.001000, ave loss: 1.15350340252
2017-03-21 13:42:24,112 INFO 	loss: 1.153503
2017-03-21 13:42:24,112 INFO 	accuracy_top1: 0.594469
2017-03-21 13:42:25,075 INFO train: iter: 6500, lr: 0.001000, ave loss: 1.13862137654
2017-03-21 13:42:25,075 INFO 	loss: 1.138621
2017-03-21 13:42:25,075 INFO 	accuracy_top1: 0.599688
2017-03-21 13:42:25,201 INFO val: iter: 6500, lr: 0.001000, ave loss: 1.19129965603
2017-03-21 13:42:25,201 INFO 	loss: 1.191300
2017-03-21 13:42:25,201 INFO 	accuracy_top1: 0.574800
2017-03-21 13:42:26,123 INFO train: iter: 6750, lr: 0.001000, ave loss: 1.12543624488
2017-03-21 13:42:26,124 INFO 	loss: 1.125436
2017-03-21 13:42:26,124 INFO 	accuracy_top1: 0.604313
2017-03-21 13:42:27,076 INFO train: iter: 7000, lr: 0.001000, ave loss: 1.11306491938
2017-03-21 13:42:27,077 INFO 	loss: 1.113065
2017-03-21 13:42:27,077 INFO 	accuracy_top1: 0.606391
2017-03-21 13:42:27,216 INFO val: iter: 7000, lr: 0.001000, ave loss: 1.17868510708
2017-03-21 13:42:27,216 INFO 	loss: 1.178685
2017-03-21 13:42:27,216 INFO 	accuracy_top1: 0.580100
2017-03-21 13:42:28,219 INFO train: iter: 7250, lr: 0.001000, ave loss: 1.11177498767
2017-03-21 13:42:28,220 INFO 	loss: 1.111775
2017-03-21 13:42:28,220 INFO 	accuracy_top1: 0.610016
2017-03-21 13:42:29,208 INFO train: iter: 7500, lr: 0.001000, ave loss: 1.09236782175
2017-03-21 13:42:29,209 INFO 	loss: 1.092368
2017-03-21 13:42:29,209 INFO 	accuracy_top1: 0.612969
2017-03-21 13:42:29,365 INFO val: iter: 7500, lr: 0.001000, ave loss: 1.16884628236
2017-03-21 13:42:29,365 INFO 	loss: 1.168846
2017-03-21 13:42:29,365 INFO 	accuracy_top1: 0.585500
2017-03-21 13:42:30,382 INFO train: iter: 7750, lr: 0.001000, ave loss: 1.09722103512
2017-03-21 13:42:30,382 INFO 	loss: 1.097221
2017-03-21 13:42:30,382 INFO 	accuracy_top1: 0.615609
2017-03-21 13:42:31,329 INFO train: iter: 8000, lr: 0.000100, ave loss: 1.07264650847
2017-03-21 13:42:31,329 INFO 	loss: 1.072647
2017-03-21 13:42:31,329 INFO 	accuracy_top1: 0.622750
2017-03-21 13:42:31,479 INFO val: iter: 8000, lr: 0.000100, ave loss: 1.15681983382
2017-03-21 13:42:31,480 INFO 	loss: 1.156820
2017-03-21 13:42:31,480 INFO 	accuracy_top1: 0.590000
2017-03-21 13:42:32,460 INFO train: iter: 8250, lr: 0.000100, ave loss: 1.06366578338
2017-03-21 13:42:32,460 INFO 	loss: 1.063666
2017-03-21 13:42:32,460 INFO 	accuracy_top1: 0.627062
2017-03-21 13:42:33,392 INFO train: iter: 8500, lr: 0.000100, ave loss: 1.06273202217
2017-03-21 13:42:33,392 INFO 	loss: 1.062732
2017-03-21 13:42:33,392 INFO 	accuracy_top1: 0.628016
2017-03-21 13:42:33,523 INFO val: iter: 8500, lr: 0.000100, ave loss: 1.14281013459
2017-03-21 13:42:33,524 INFO 	loss: 1.142810
2017-03-21 13:42:33,524 INFO 	accuracy_top1: 0.597100
2017-03-21 13:42:34,503 INFO train: iter: 8750, lr: 0.000100, ave loss: 1.06600320506
2017-03-21 13:42:34,504 INFO 	loss: 1.066003
2017-03-21 13:42:34,504 INFO 	accuracy_top1: 0.627016
2017-03-21 13:42:35,419 INFO train: iter: 9000, lr: 0.000100, ave loss: 1.06153462872
2017-03-21 13:42:35,419 INFO 	loss: 1.061535
2017-03-21 13:42:35,419 INFO 	accuracy_top1: 0.629656
2017-03-21 13:42:35,523 INFO val: iter: 9000, lr: 0.000100, ave loss: 1.14064788595
2017-03-21 13:42:35,523 INFO 	loss: 1.140648
2017-03-21 13:42:35,524 INFO 	accuracy_top1: 0.597200
2017-03-21 13:42:36,398 INFO train: iter: 9250, lr: 0.000100, ave loss: 1.06088475758
2017-03-21 13:42:36,399 INFO 	loss: 1.060885
2017-03-21 13:42:36,399 INFO 	accuracy_top1: 0.627906
2017-03-21 13:42:37,237 INFO train: iter: 9500, lr: 0.000100, ave loss: 1.05602212769
2017-03-21 13:42:37,238 INFO 	loss: 1.056022
2017-03-21 13:42:37,238 INFO 	accuracy_top1: 0.628687
2017-03-21 13:42:37,356 INFO val: iter: 9500, lr: 0.000100, ave loss: 1.14094649851
2017-03-21 13:42:37,356 INFO 	loss: 1.140946
2017-03-21 13:42:37,357 INFO 	accuracy_top1: 0.597100
2017-03-21 13:42:38,287 INFO train: iter: 9750, lr: 0.000100, ave loss: 1.0570436554
2017-03-21 13:42:38,287 INFO 	loss: 1.057044
2017-03-21 13:42:38,287 INFO 	accuracy_top1: 0.629719
2017-03-21 13:42:39,166 INFO train: iter: 10000, lr: 0.000010, ave loss: 1.05698974743
2017-03-21 13:42:39,166 INFO 	loss: 1.056990
2017-03-21 13:42:39,166 INFO 	accuracy_top1: 0.629016
2017-03-21 13:42:39,292 INFO val: iter: 10000, lr: 0.000010, ave loss: 1.13963319659
2017-03-21 13:42:39,293 INFO 	loss: 1.139633
2017-03-21 13:42:39,293 INFO 	accuracy_top1: 0.599200
2017-03-21 13:42:40,168 INFO train: iter: 10250, lr: 0.000010, ave loss: 1.05503851286
2017-03-21 13:42:40,168 INFO 	loss: 1.055039
2017-03-21 13:42:40,168 INFO 	accuracy_top1: 0.627563
2017-03-21 13:42:41,045 INFO train: iter: 10500, lr: 0.000010, ave loss: 1.05349513485
2017-03-21 13:42:41,045 INFO 	loss: 1.053495
2017-03-21 13:42:41,045 INFO 	accuracy_top1: 0.631500
2017-03-21 13:42:41,174 INFO val: iter: 10500, lr: 0.000010, ave loss: 1.13886545673
2017-03-21 13:42:41,174 INFO 	loss: 1.138865
2017-03-21 13:42:41,174 INFO 	accuracy_top1: 0.598200
2017-03-21 13:42:42,049 INFO train: iter: 10750, lr: 0.000010, ave loss: 1.05743577695
2017-03-21 13:42:42,049 INFO 	loss: 1.057436
2017-03-21 13:42:42,049 INFO 	accuracy_top1: 0.631328
2017-03-21 13:42:42,977 INFO train: iter: 11000, lr: 0.000010, ave loss: 1.05240563276
2017-03-21 13:42:42,977 INFO 	loss: 1.052406
2017-03-21 13:42:42,977 INFO 	accuracy_top1: 0.629016
2017-03-21 13:42:43,116 INFO val: iter: 11000, lr: 0.000010, ave loss: 1.13855773211
2017-03-21 13:42:43,116 INFO 	loss: 1.138558
2017-03-21 13:42:43,116 INFO 	accuracy_top1: 0.597500
2017-03-21 13:42:44,006 INFO train: iter: 11250, lr: 0.000010, ave loss: 1.04456792048
2017-03-21 13:42:44,006 INFO 	loss: 1.044568
2017-03-21 13:42:44,006 INFO 	accuracy_top1: 0.634625
2017-03-21 13:42:44,839 INFO train: iter: 11500, lr: 0.000010, ave loss: 1.05200985995
2017-03-21 13:42:44,839 INFO 	loss: 1.052010
2017-03-21 13:42:44,839 INFO 	accuracy_top1: 0.631422
2017-03-21 13:42:44,959 INFO val: iter: 11500, lr: 0.000010, ave loss: 1.13824057281
2017-03-21 13:42:44,960 INFO 	loss: 1.138241
2017-03-21 13:42:44,960 INFO 	accuracy_top1: 0.598700
2017-03-21 13:42:45,787 INFO train: iter: 11750, lr: 0.000010, ave loss: 1.05157089096
2017-03-21 13:42:45,787 INFO 	loss: 1.051571
2017-03-21 13:42:45,788 INFO 	accuracy_top1: 0.630219
2017-03-21 13:42:46,755 INFO train: iter: 12000, lr: 0.000010, ave loss: 1.05535992175
2017-03-21 13:42:46,755 INFO 	loss: 1.055360
2017-03-21 13:42:46,755 INFO 	accuracy_top1: 0.628219
2017-03-21 13:42:46,870 INFO val: iter: 12000, lr: 0.000010, ave loss: 1.13823433965
2017-03-21 13:42:46,870 INFO 	loss: 1.138234
2017-03-21 13:42:46,871 INFO 	accuracy_top1: 0.597800
2017-03-21 13:43:06,579 INFO connecting pavi service http://pavi.parrotsdnn.org/log
2017-03-21 13:43:06,615 INFO pavi service connected, instance_id: 3e3d5f4de3c644838f1dcc9fc5745a10
2017-03-21 13:43:06,639 INFO model name: net1_msra
2017-03-21 13:43:06,640 DEBUG name: "net1_msra"
inputs:
  - { id: data , spec: "Float32(32, 32, 3, _)" }
  - { id: label, spec: "Uint32(1, _)" }
params:
  - { id: conv1.w, spec: "Float32(5, 5, 3, 6)" , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: conv2.w, spec: "Float32(5, 5, 6, 16)", learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.w  , spec: "Float32(400, 120)"   , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.b  , spec: "Float32(120, 1)"     , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc2.w  , spec: "Float32(120, 84)"    , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc2.b  , spec: "Float32(84, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc.w   , spec: "Float32(84, 10)"     , learning-policy: { init: msra, lr_mult: 1, decay_mult: 1 } }
  - { id: fc.b   , spec: "Float32(10, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
layers:
  - { id: conv1, expr: "conv1 = Convolution(data, @conv1.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 6, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu1, expr: "conv1 = ReLU(conv1)" }
  - { id: pool1, expr: "pool1 = Pooling(conv1)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool1relu, expr: "pool1 = ReLU(pool1)" }
  - { id: conv2, expr: "conv2 = Convolution(pool1, @conv2.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 16, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu2, expr: "conv2 = ReLU(conv2)" }
  - { id: pool2, expr: "pool2 = Pooling(conv2)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool2relu, expr: "pool2 = ReLU(pool2)" }
  - { id: fc1, expr: "fc1 = FullyConnected(pool2, @fc1.w, @fc1.b)",
      attrs: {slice_axis: -2, num_output: 120} }
  - { id: fc1relu, expr: "fc1 = ReLU(fc1)" }
  - { id: fc2, expr: "fc2 = FullyConnected(fc1, @fc2.w, @fc2.b)",
      attrs: {slice_axis: -2, num_output: 84} }
  - { id: fc2relu, expr: "fc2 = ReLU(fc2)" }
  - { id: fc, expr: "fc = FullyConnected(fc2, @fc.w, @fc.b)",
      attrs: {slice_axis: -2, num_output: 10} }
  - { id: loss, expr: "loss = SoftmaxWithLoss(fc, label)",
      attrs: {axis: 0} }
  - { id: accuracy_top1, expr: "accuracy_top1 = Accuracy(fc, label)",
      attrs: {top_k: 1, slice_axis: 0} }
flows:
  - { name: main, inputs: [data, label], outputs: [loss, accuracy_top1], losses: [loss * 1, accuracy_top1 * 1] }

2017-03-21 13:43:08,661 INFO train: iter: 250, lr: 0.001000, ave loss: 2.13127528065
2017-03-21 13:43:08,662 INFO 	loss: 2.131275
2017-03-21 13:43:08,662 INFO 	accuracy_top1: 0.225047
2017-03-21 13:43:09,651 INFO train: iter: 500, lr: 0.001000, ave loss: 1.87611188519
2017-03-21 13:43:09,651 INFO 	loss: 1.876112
2017-03-21 13:43:09,652 INFO 	accuracy_top1: 0.328188
2017-03-21 13:43:09,836 INFO val: iter: 500, lr: 0.001000, ave loss: 1.79507556707
2017-03-21 13:43:09,836 INFO 	loss: 1.795076
2017-03-21 13:43:09,836 INFO 	accuracy_top1: 0.358100
2017-03-21 13:43:10,738 INFO train: iter: 750, lr: 0.001000, ave loss: 1.73756943172
2017-03-21 13:43:10,739 INFO 	loss: 1.737569
2017-03-21 13:43:10,739 INFO 	accuracy_top1: 0.379641
2017-03-21 13:43:11,588 INFO train: iter: 1000, lr: 0.001000, ave loss: 1.63194101006
2017-03-21 13:43:11,588 INFO 	loss: 1.631941
2017-03-21 13:43:11,588 INFO 	accuracy_top1: 0.412891
2017-03-21 13:43:11,716 INFO val: iter: 1000, lr: 0.001000, ave loss: 1.59813564867
2017-03-21 13:43:11,716 INFO 	loss: 1.598136
2017-03-21 13:43:11,716 INFO 	accuracy_top1: 0.425200
2017-03-21 13:43:12,629 INFO train: iter: 1250, lr: 0.001000, ave loss: 1.5774000867
2017-03-21 13:43:12,629 INFO 	loss: 1.577400
2017-03-21 13:43:12,629 INFO 	accuracy_top1: 0.431391
2017-03-21 13:43:13,544 INFO train: iter: 1500, lr: 0.001000, ave loss: 1.52283064175
2017-03-21 13:43:13,544 INFO 	loss: 1.522831
2017-03-21 13:43:13,544 INFO 	accuracy_top1: 0.455188
2017-03-21 13:43:13,688 INFO val: iter: 1500, lr: 0.001000, ave loss: 1.5137866348
2017-03-21 13:43:13,689 INFO 	loss: 1.513787
2017-03-21 13:43:13,689 INFO 	accuracy_top1: 0.460500
2017-03-21 13:43:14,570 INFO train: iter: 1750, lr: 0.001000, ave loss: 1.48753520858
2017-03-21 13:43:14,570 INFO 	loss: 1.487535
2017-03-21 13:43:14,570 INFO 	accuracy_top1: 0.467141
2017-03-21 13:43:15,531 INFO train: iter: 2000, lr: 0.001000, ave loss: 1.46123509905
2017-03-21 13:43:15,532 INFO 	loss: 1.461235
2017-03-21 13:43:15,532 INFO 	accuracy_top1: 0.478891
2017-03-21 13:43:15,664 INFO val: iter: 2000, lr: 0.001000, ave loss: 1.45840813518
2017-03-21 13:43:15,664 INFO 	loss: 1.458408
2017-03-21 13:43:15,664 INFO 	accuracy_top1: 0.480200
2017-03-21 13:43:16,628 INFO train: iter: 2250, lr: 0.001000, ave loss: 1.43186647293
2017-03-21 13:43:16,629 INFO 	loss: 1.431866
2017-03-21 13:43:16,629 INFO 	accuracy_top1: 0.489156
2017-03-21 13:43:17,496 INFO train: iter: 2500, lr: 0.001000, ave loss: 1.4093424367
2017-03-21 13:43:17,497 INFO 	loss: 1.409342
2017-03-21 13:43:17,497 INFO 	accuracy_top1: 0.499437
2017-03-21 13:43:17,691 INFO val: iter: 2500, lr: 0.001000, ave loss: 1.42234831303
2017-03-21 13:43:17,692 INFO 	loss: 1.422348
2017-03-21 13:43:17,692 INFO 	accuracy_top1: 0.498600
2017-03-21 13:43:18,669 INFO train: iter: 2750, lr: 0.001000, ave loss: 1.38788734043
2017-03-21 13:43:18,669 INFO 	loss: 1.387887
2017-03-21 13:43:18,669 INFO 	accuracy_top1: 0.508016
2017-03-21 13:43:19,532 INFO train: iter: 3000, lr: 0.001000, ave loss: 1.36541983619
2017-03-21 13:43:19,533 INFO 	loss: 1.365420
2017-03-21 13:43:19,533 INFO 	accuracy_top1: 0.517859
2017-03-21 13:43:19,666 INFO val: iter: 3000, lr: 0.001000, ave loss: 1.39349143952
2017-03-21 13:43:19,666 INFO 	loss: 1.393491
2017-03-21 13:43:19,666 INFO 	accuracy_top1: 0.506700
2017-03-21 13:43:20,585 INFO train: iter: 3250, lr: 0.001000, ave loss: 1.35360509259
2017-03-21 13:43:20,585 INFO 	loss: 1.353605
2017-03-21 13:43:20,585 INFO 	accuracy_top1: 0.519844
2017-03-21 13:43:21,488 INFO train: iter: 3500, lr: 0.001000, ave loss: 1.32604911736
2017-03-21 13:43:21,489 INFO 	loss: 1.326049
2017-03-21 13:43:21,489 INFO 	accuracy_top1: 0.534922
2017-03-21 13:43:21,617 INFO val: iter: 3500, lr: 0.001000, ave loss: 1.35127266794
2017-03-21 13:43:21,618 INFO 	loss: 1.351273
2017-03-21 13:43:21,618 INFO 	accuracy_top1: 0.528500
2017-03-21 13:43:22,550 INFO train: iter: 3750, lr: 0.001000, ave loss: 1.30547471917
2017-03-21 13:43:22,550 INFO 	loss: 1.305475
2017-03-21 13:43:22,550 INFO 	accuracy_top1: 0.541656
2017-03-21 13:43:23,576 INFO train: iter: 4000, lr: 0.001000, ave loss: 1.29352145404
2017-03-21 13:43:23,576 INFO 	loss: 1.293521
2017-03-21 13:43:23,576 INFO 	accuracy_top1: 0.547656
2017-03-21 13:43:23,731 INFO val: iter: 4000, lr: 0.001000, ave loss: 1.31772161573
2017-03-21 13:43:23,731 INFO 	loss: 1.317722
2017-03-21 13:43:23,732 INFO 	accuracy_top1: 0.539400
2017-03-21 13:43:24,641 INFO train: iter: 4250, lr: 0.001000, ave loss: 1.28080454963
2017-03-21 13:43:24,642 INFO 	loss: 1.280805
2017-03-21 13:43:24,642 INFO 	accuracy_top1: 0.550859
2017-03-21 13:43:25,510 INFO train: iter: 4500, lr: 0.001000, ave loss: 1.25913840473
2017-03-21 13:43:25,510 INFO 	loss: 1.259138
2017-03-21 13:43:25,510 INFO 	accuracy_top1: 0.562016
2017-03-21 13:43:25,648 INFO val: iter: 4500, lr: 0.001000, ave loss: 1.28985706568
2017-03-21 13:43:25,649 INFO 	loss: 1.289857
2017-03-21 13:43:25,649 INFO 	accuracy_top1: 0.547500
2017-03-21 13:43:26,609 INFO train: iter: 4750, lr: 0.001000, ave loss: 1.24863567591
2017-03-21 13:43:26,609 INFO 	loss: 1.248636
2017-03-21 13:43:26,609 INFO 	accuracy_top1: 0.563000
2017-03-21 13:43:27,480 INFO train: iter: 5000, lr: 0.001000, ave loss: 1.23933234891
2017-03-21 13:43:27,480 INFO 	loss: 1.239332
2017-03-21 13:43:27,480 INFO 	accuracy_top1: 0.568594
2017-03-21 13:43:27,616 INFO val: iter: 5000, lr: 0.001000, ave loss: 1.26889751107
2017-03-21 13:43:27,617 INFO 	loss: 1.268898
2017-03-21 13:43:27,617 INFO 	accuracy_top1: 0.558700
2017-03-21 13:43:28,521 INFO train: iter: 5250, lr: 0.001000, ave loss: 1.22138900876
2017-03-21 13:43:28,521 INFO 	loss: 1.221389
2017-03-21 13:43:28,521 INFO 	accuracy_top1: 0.571125
2017-03-21 13:43:29,449 INFO train: iter: 5500, lr: 0.001000, ave loss: 1.2089631249
2017-03-21 13:43:29,449 INFO 	loss: 1.208963
2017-03-21 13:43:29,449 INFO 	accuracy_top1: 0.575844
2017-03-21 13:43:29,572 INFO val: iter: 5500, lr: 0.001000, ave loss: 1.26967194974
2017-03-21 13:43:29,573 INFO 	loss: 1.269672
2017-03-21 13:43:29,573 INFO 	accuracy_top1: 0.556000
2017-03-21 13:43:30,447 INFO train: iter: 5750, lr: 0.001000, ave loss: 1.19825491121
2017-03-21 13:43:30,447 INFO 	loss: 1.198255
2017-03-21 13:43:30,447 INFO 	accuracy_top1: 0.579578
2017-03-21 13:43:31,373 INFO train: iter: 6000, lr: 0.001000, ave loss: 1.18513494495
2017-03-21 13:43:31,373 INFO 	loss: 1.185135
2017-03-21 13:43:31,373 INFO 	accuracy_top1: 0.585484
2017-03-21 13:43:31,482 INFO val: iter: 6000, lr: 0.001000, ave loss: 1.232473436
2017-03-21 13:43:31,482 INFO 	loss: 1.232473
2017-03-21 13:43:31,482 INFO 	accuracy_top1: 0.568600
2017-03-21 13:43:32,372 INFO train: iter: 6250, lr: 0.001000, ave loss: 1.18182998759
2017-03-21 13:43:32,372 INFO 	loss: 1.181830
2017-03-21 13:43:32,373 INFO 	accuracy_top1: 0.588109
2017-03-21 13:43:33,244 INFO train: iter: 6500, lr: 0.001000, ave loss: 1.17054666698
2017-03-21 13:43:33,245 INFO 	loss: 1.170547
2017-03-21 13:43:33,245 INFO 	accuracy_top1: 0.592156
2017-03-21 13:43:33,379 INFO val: iter: 6500, lr: 0.001000, ave loss: 1.23852822036
2017-03-21 13:43:33,379 INFO 	loss: 1.238528
2017-03-21 13:43:33,379 INFO 	accuracy_top1: 0.565800
2017-03-21 13:43:34,357 INFO train: iter: 6750, lr: 0.001000, ave loss: 1.15844356298
2017-03-21 13:43:34,357 INFO 	loss: 1.158444
2017-03-21 13:43:34,357 INFO 	accuracy_top1: 0.599250
2017-03-21 13:43:35,348 INFO train: iter: 7000, lr: 0.001000, ave loss: 1.15242544696
2017-03-21 13:43:35,348 INFO 	loss: 1.152425
2017-03-21 13:43:35,348 INFO 	accuracy_top1: 0.598844
2017-03-21 13:43:35,514 INFO val: iter: 7000, lr: 0.001000, ave loss: 1.23268598244
2017-03-21 13:43:35,515 INFO 	loss: 1.232686
2017-03-21 13:43:35,515 INFO 	accuracy_top1: 0.571300
2017-03-21 13:43:36,580 INFO train: iter: 7250, lr: 0.001000, ave loss: 1.14651039648
2017-03-21 13:43:36,580 INFO 	loss: 1.146510
2017-03-21 13:43:36,580 INFO 	accuracy_top1: 0.600656
2017-03-21 13:43:37,584 INFO train: iter: 7500, lr: 0.001000, ave loss: 1.14018619668
2017-03-21 13:43:37,584 INFO 	loss: 1.140186
2017-03-21 13:43:37,584 INFO 	accuracy_top1: 0.601797
2017-03-21 13:43:37,737 INFO val: iter: 7500, lr: 0.001000, ave loss: 1.19168053269
2017-03-21 13:43:37,737 INFO 	loss: 1.191681
2017-03-21 13:43:37,737 INFO 	accuracy_top1: 0.586400
2017-03-21 13:43:38,774 INFO train: iter: 7750, lr: 0.001000, ave loss: 1.12180590045
2017-03-21 13:43:38,774 INFO 	loss: 1.121806
2017-03-21 13:43:38,774 INFO 	accuracy_top1: 0.608828
2017-03-21 13:43:39,714 INFO train: iter: 8000, lr: 0.000100, ave loss: 1.11387875971
2017-03-21 13:43:39,714 INFO 	loss: 1.113879
2017-03-21 13:43:39,714 INFO 	accuracy_top1: 0.612797
2017-03-21 13:43:39,866 INFO val: iter: 8000, lr: 0.000100, ave loss: 1.19524898008
2017-03-21 13:43:39,866 INFO 	loss: 1.195249
2017-03-21 13:43:39,866 INFO 	accuracy_top1: 0.586200
2017-03-21 13:43:40,865 INFO train: iter: 8250, lr: 0.000100, ave loss: 1.10369279137
2017-03-21 13:43:40,865 INFO 	loss: 1.103693
2017-03-21 13:43:40,865 INFO 	accuracy_top1: 0.614078
2017-03-21 13:43:41,908 INFO train: iter: 8500, lr: 0.000100, ave loss: 1.09961149213
2017-03-21 13:43:41,908 INFO 	loss: 1.099611
2017-03-21 13:43:41,908 INFO 	accuracy_top1: 0.618250
2017-03-21 13:43:42,061 INFO val: iter: 8500, lr: 0.000100, ave loss: 1.17047673389
2017-03-21 13:43:42,061 INFO 	loss: 1.170477
2017-03-21 13:43:42,061 INFO 	accuracy_top1: 0.593400
2017-03-21 13:43:43,011 INFO train: iter: 8750, lr: 0.000100, ave loss: 1.09588808903
2017-03-21 13:43:43,011 INFO 	loss: 1.095888
2017-03-21 13:43:43,011 INFO 	accuracy_top1: 0.617656
2017-03-21 13:43:43,977 INFO train: iter: 9000, lr: 0.000100, ave loss: 1.09268599766
2017-03-21 13:43:43,978 INFO 	loss: 1.092686
2017-03-21 13:43:43,978 INFO 	accuracy_top1: 0.621422
2017-03-21 13:43:44,135 INFO val: iter: 9000, lr: 0.000100, ave loss: 1.16800500676
2017-03-21 13:43:44,136 INFO 	loss: 1.168005
2017-03-21 13:43:44,136 INFO 	accuracy_top1: 0.593700
2017-03-21 13:43:45,158 INFO train: iter: 9250, lr: 0.000100, ave loss: 1.09068504092
2017-03-21 13:43:45,158 INFO 	loss: 1.090685
2017-03-21 13:43:45,158 INFO 	accuracy_top1: 0.619016
2017-03-21 13:43:46,092 INFO train: iter: 9500, lr: 0.000100, ave loss: 1.09755551654
2017-03-21 13:43:46,092 INFO 	loss: 1.097556
2017-03-21 13:43:46,092 INFO 	accuracy_top1: 0.619563
2017-03-21 13:43:46,240 INFO val: iter: 9500, lr: 0.000100, ave loss: 1.16561541334
2017-03-21 13:43:46,240 INFO 	loss: 1.165615
2017-03-21 13:43:46,241 INFO 	accuracy_top1: 0.594700
2017-03-21 13:43:47,145 INFO train: iter: 9750, lr: 0.000100, ave loss: 1.09292426872
2017-03-21 13:43:47,145 INFO 	loss: 1.092924
2017-03-21 13:43:47,145 INFO 	accuracy_top1: 0.619563
2017-03-21 13:43:48,064 INFO train: iter: 10000, lr: 0.000010, ave loss: 1.09339467105
2017-03-21 13:43:48,064 INFO 	loss: 1.093395
2017-03-21 13:43:48,064 INFO 	accuracy_top1: 0.618484
2017-03-21 13:43:48,221 INFO val: iter: 10000, lr: 0.000010, ave loss: 1.16893918812
2017-03-21 13:43:48,222 INFO 	loss: 1.168939
2017-03-21 13:43:48,222 INFO 	accuracy_top1: 0.595400
2017-03-21 13:43:49,145 INFO train: iter: 10250, lr: 0.000010, ave loss: 1.08547083876
2017-03-21 13:43:49,145 INFO 	loss: 1.085471
2017-03-21 13:43:49,145 INFO 	accuracy_top1: 0.624172
2017-03-21 13:43:50,033 INFO train: iter: 10500, lr: 0.000010, ave loss: 1.08626317221
2017-03-21 13:43:50,033 INFO 	loss: 1.086263
2017-03-21 13:43:50,033 INFO 	accuracy_top1: 0.621734
2017-03-21 13:43:50,179 INFO val: iter: 10500, lr: 0.000010, ave loss: 1.16350124702
2017-03-21 13:43:50,180 INFO 	loss: 1.163501
2017-03-21 13:43:50,180 INFO 	accuracy_top1: 0.596200
2017-03-21 13:43:51,108 INFO train: iter: 10750, lr: 0.000010, ave loss: 1.08802579296
2017-03-21 13:43:51,109 INFO 	loss: 1.088026
2017-03-21 13:43:51,109 INFO 	accuracy_top1: 0.622531
2017-03-21 13:43:52,065 INFO train: iter: 11000, lr: 0.000010, ave loss: 1.08639871705
2017-03-21 13:43:52,065 INFO 	loss: 1.086399
2017-03-21 13:43:52,065 INFO 	accuracy_top1: 0.622641
2017-03-21 13:43:52,205 INFO val: iter: 11000, lr: 0.000010, ave loss: 1.16475651637
2017-03-21 13:43:52,206 INFO 	loss: 1.164757
2017-03-21 13:43:52,206 INFO 	accuracy_top1: 0.594900
2017-03-21 13:43:53,140 INFO train: iter: 11250, lr: 0.000010, ave loss: 1.08593847156
2017-03-21 13:43:53,140 INFO 	loss: 1.085938
2017-03-21 13:43:53,140 INFO 	accuracy_top1: 0.622844
2017-03-21 13:43:54,029 INFO train: iter: 11500, lr: 0.000010, ave loss: 1.09419934168
2017-03-21 13:43:54,029 INFO 	loss: 1.094199
2017-03-21 13:43:54,030 INFO 	accuracy_top1: 0.620844
2017-03-21 13:43:54,165 INFO val: iter: 11500, lr: 0.000010, ave loss: 1.16381326765
2017-03-21 13:43:54,165 INFO 	loss: 1.163813
2017-03-21 13:43:54,165 INFO 	accuracy_top1: 0.596000
2017-03-21 13:43:55,102 INFO train: iter: 11750, lr: 0.000010, ave loss: 1.08795419341
2017-03-21 13:43:55,102 INFO 	loss: 1.087954
2017-03-21 13:43:55,102 INFO 	accuracy_top1: 0.622375
2017-03-21 13:43:56,067 INFO train: iter: 12000, lr: 0.000010, ave loss: 1.08733508986
2017-03-21 13:43:56,067 INFO 	loss: 1.087335
2017-03-21 13:43:56,067 INFO 	accuracy_top1: 0.624313
2017-03-21 13:43:56,195 INFO val: iter: 12000, lr: 0.000010, ave loss: 1.16404363811
2017-03-21 13:43:56,195 INFO 	loss: 1.164044
2017-03-21 13:43:56,195 INFO 	accuracy_top1: 0.595300
2017-03-21 13:45:54,472 INFO connecting pavi service http://pavi.parrotsdnn.org/log
2017-03-21 13:45:54,504 INFO pavi service connected, instance_id: b16caa6124f84dabbfcd7430d7ebf884
2017-03-21 13:45:54,525 INFO model name: net1_msra
2017-03-21 13:45:54,526 DEBUG name: "net1_msra"
inputs:
  - { id: data , spec: "Float32(32, 32, 3, _)" }
  - { id: label, spec: "Uint32(1, _)" }
params:
  - { id: conv1.w, spec: "Float32(5, 5, 3, 6)" , learning-policy: { init: gauss(0.01), lr_mult: 1, decay_mult: 1 } }
  - { id: conv2.w, spec: "Float32(5, 5, 6, 16)", learning-policy: { init: gauss(0.01), lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.w  , spec: "Float32(400, 120)"   , learning-policy: { init: gauss(0.01), lr_mult: 1, decay_mult: 1 } }
  - { id: fc1.b  , spec: "Float32(120, 1)"     , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc2.w  , spec: "Float32(120, 84)"    , learning-policy: { init: gauss(0.01), lr_mult: 1, decay_mult: 1 } }
  - { id: fc2.b  , spec: "Float32(84, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
  - { id: fc.w   , spec: "Float32(84, 10)"     , learning-policy: { init: gauss(0.01), lr_mult: 1, decay_mult: 1 } }
  - { id: fc.b   , spec: "Float32(10, 1)"      , learning-policy: { init: fill(0), lr_mult: 2, decay_mult: 0 } }
layers:
  - { id: conv1, expr: "conv1 = Convolution(data, @conv1.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 6, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu1, expr: "conv1 = ReLU(conv1)" }
  - { id: pool1, expr: "pool1 = Pooling(conv1)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool1relu, expr: "pool1 = ReLU(pool1)" }
  - { id: conv2, expr: "conv2 = Convolution(pool1, @conv2.w)",
      attrs: {kernel_w: 5, pad_h: 0, hole_w: 1, hole_h: 1, num_output: 16, stride_w: 1, stride_h: 1, pad_w: 0, kernel_h: 5} }
  - { id: relu2, expr: "conv2 = ReLU(conv2)" }
  - { id: pool2, expr: "pool2 = Pooling(conv2)",
      attrs: {kernel_w: 3, pad_h: 0, stride_h: 2, stride_w: 2, mode: max, pad_w: 0, kernel_h: 3} }
  - { id: pool2relu, expr: "pool2 = ReLU(pool2)" }
  - { id: fc1, expr: "fc1 = FullyConnected(pool2, @fc1.w, @fc1.b)",
      attrs: {slice_axis: -2, num_output: 120} }
  - { id: fc1relu, expr: "fc1 = ReLU(fc1)" }
  - { id: fc2, expr: "fc2 = FullyConnected(fc1, @fc2.w, @fc2.b)",
      attrs: {slice_axis: -2, num_output: 84} }
  - { id: fc2relu, expr: "fc2 = ReLU(fc2)" }
  - { id: fc, expr: "fc = FullyConnected(fc2, @fc.w, @fc.b)",
      attrs: {slice_axis: -2, num_output: 10} }
  - { id: loss, expr: "loss = SoftmaxWithLoss(fc, label)",
      attrs: {axis: 0} }
  - { id: accuracy_top1, expr: "accuracy_top1 = Accuracy(fc, label)",
      attrs: {top_k: 1, slice_axis: 0} }
flows:
  - { name: main, inputs: [data, label], outputs: [loss, accuracy_top1], losses: [loss * 1, accuracy_top1 * 1] }

2017-03-21 13:45:56,257 INFO train: iter: 250, lr: 0.001000, ave loss: 2.30259696257
2017-03-21 13:45:56,257 INFO 	loss: 2.302597
2017-03-21 13:45:56,258 INFO 	accuracy_top1: 0.099734
2017-03-21 13:45:57,261 INFO train: iter: 500, lr: 0.001000, ave loss: 2.30261055505
2017-03-21 13:45:57,262 INFO 	loss: 2.302611
2017-03-21 13:45:57,262 INFO 	accuracy_top1: 0.098172
2017-03-21 13:45:57,419 INFO val: iter: 500, lr: 0.001000, ave loss: 2.30259288847
2017-03-21 13:45:57,419 INFO 	loss: 2.302593
2017-03-21 13:45:57,419 INFO 	accuracy_top1: 0.100000
2017-03-21 13:45:58,429 INFO train: iter: 750, lr: 0.001000, ave loss: 2.30260966158
2017-03-21 13:45:58,429 INFO 	loss: 2.302610
2017-03-21 13:45:58,429 INFO 	accuracy_top1: 0.097562
2017-03-21 13:45:59,238 INFO train: iter: 1000, lr: 0.001000, ave loss: 2.30261971855
2017-03-21 13:45:59,238 INFO 	loss: 2.302620
2017-03-21 13:45:59,238 INFO 	accuracy_top1: 0.098078
2017-03-21 13:45:59,352 INFO val: iter: 1000, lr: 0.001000, ave loss: 2.30258898139
2017-03-21 13:45:59,353 INFO 	loss: 2.302589
2017-03-21 13:45:59,353 INFO 	accuracy_top1: 0.100000
2017-03-21 13:46:00,236 INFO train: iter: 1250, lr: 0.001000, ave loss: 2.302597031
2017-03-21 13:46:00,236 INFO 	loss: 2.302597
2017-03-21 13:46:00,237 INFO 	accuracy_top1: 0.100219
2017-03-21 13:46:01,152 INFO train: iter: 1500, lr: 0.001000, ave loss: 2.30261230838
2017-03-21 13:46:01,152 INFO 	loss: 2.302612
2017-03-21 13:46:01,152 INFO 	accuracy_top1: 0.099187
2017-03-21 13:46:01,324 INFO val: iter: 1500, lr: 0.001000, ave loss: 2.3025891006
2017-03-21 13:46:01,324 INFO 	loss: 2.302589
2017-03-21 13:46:01,325 INFO 	accuracy_top1: 0.100000
2017-03-21 13:46:02,269 INFO train: iter: 1750, lr: 0.001000, ave loss: 2.3026075002
2017-03-21 13:46:02,270 INFO 	loss: 2.302608
2017-03-21 13:46:02,270 INFO 	accuracy_top1: 0.101234
2017-03-21 13:46:03,238 INFO train: iter: 2000, lr: 0.001000, ave loss: 2.30260619938
2017-03-21 13:46:03,238 INFO 	loss: 2.302606
2017-03-21 13:46:03,238 INFO 	accuracy_top1: 0.099609
2017-03-21 13:46:03,370 INFO val: iter: 2000, lr: 0.001000, ave loss: 2.30259467959
2017-03-21 13:46:03,371 INFO 	loss: 2.302595
2017-03-21 13:46:03,371 INFO 	accuracy_top1: 0.100000
2017-03-21 13:46:04,282 INFO train: iter: 2250, lr: 0.001000, ave loss: 2.30257591343
2017-03-21 13:46:04,282 INFO 	loss: 2.302576
2017-03-21 13:46:04,283 INFO 	accuracy_top1: 0.102234
2017-03-21 13:46:05,193 INFO train: iter: 2500, lr: 0.001000, ave loss: 2.30262073874
2017-03-21 13:46:05,193 INFO 	loss: 2.302621
2017-03-21 13:46:05,193 INFO 	accuracy_top1: 0.099484
2017-03-21 13:46:05,334 INFO val: iter: 2500, lr: 0.001000, ave loss: 2.30259796381
2017-03-21 13:46:05,334 INFO 	loss: 2.302598
2017-03-21 13:46:05,334 INFO 	accuracy_top1: 0.100000
2017-03-21 13:46:06,282 INFO train: iter: 2750, lr: 0.001000, ave loss: 2.30260334718
2017-03-21 13:46:06,282 INFO 	loss: 2.302603
2017-03-21 13:46:06,282 INFO 	accuracy_top1: 0.100187
2017-03-21 13:46:07,199 INFO train: iter: 3000, lr: 0.001000, ave loss: 2.3025874722
2017-03-21 13:46:07,199 INFO 	loss: 2.302587
2017-03-21 13:46:07,199 INFO 	accuracy_top1: 0.099687
2017-03-21 13:46:07,324 INFO val: iter: 3000, lr: 0.001000, ave loss: 2.30260489285
2017-03-21 13:46:07,324 INFO 	loss: 2.302605
2017-03-21 13:46:07,324 INFO 	accuracy_top1: 0.100000
2017-03-21 13:46:08,251 INFO train: iter: 3250, lr: 0.001000, ave loss: 2.30259875357
2017-03-21 13:46:08,251 INFO 	loss: 2.302599
2017-03-21 13:46:08,251 INFO 	accuracy_top1: 0.100312
2017-03-21 13:46:09,131 INFO train: iter: 3500, lr: 0.001000, ave loss: 2.30263529062
2017-03-21 13:46:09,131 INFO 	loss: 2.302635
2017-03-21 13:46:09,132 INFO 	accuracy_top1: 0.099500
2017-03-21 13:46:09,255 INFO val: iter: 3500, lr: 0.001000, ave loss: 2.30258879364
2017-03-21 13:46:09,255 INFO 	loss: 2.302589
2017-03-21 13:46:09,255 INFO 	accuracy_top1: 0.100000
2017-03-21 13:46:10,156 INFO train: iter: 3750, lr: 0.001000, ave loss: 2.30261238158
2017-03-21 13:46:10,156 INFO 	loss: 2.302612
2017-03-21 13:46:10,156 INFO 	accuracy_top1: 0.099375
2017-03-21 13:46:11,092 INFO train: iter: 4000, lr: 0.001000, ave loss: 2.30259834087
2017-03-21 13:46:11,092 INFO 	loss: 2.302598
2017-03-21 13:46:11,092 INFO 	accuracy_top1: 0.100844
2017-03-21 13:46:11,228 INFO val: iter: 4000, lr: 0.001000, ave loss: 2.30259300172
2017-03-21 13:46:11,228 INFO 	loss: 2.302593
2017-03-21 13:46:11,228 INFO 	accuracy_top1: 0.100000
2017-03-21 13:46:12,191 INFO train: iter: 4250, lr: 0.001000, ave loss: 2.30259659815
2017-03-21 13:46:12,191 INFO 	loss: 2.302597
2017-03-21 13:46:12,191 INFO 	accuracy_top1: 0.100906
2017-03-21 13:46:13,119 INFO train: iter: 4500, lr: 0.001000, ave loss: 2.3025826571
2017-03-21 13:46:13,119 INFO 	loss: 2.302583
2017-03-21 13:46:13,119 INFO 	accuracy_top1: 0.101141
2017-03-21 13:46:13,275 INFO val: iter: 4500, lr: 0.001000, ave loss: 2.30260408819
2017-03-21 13:46:13,275 INFO 	loss: 2.302604
2017-03-21 13:46:13,275 INFO 	accuracy_top1: 0.100000
2017-03-21 13:46:14,241 INFO train: iter: 4750, lr: 0.001000, ave loss: 2.30262490189
2017-03-21 13:46:14,241 INFO 	loss: 2.302625
2017-03-21 13:46:14,241 INFO 	accuracy_top1: 0.098266
2017-03-21 13:46:14,774 ERROR KeyboardInterrupt
Traceback (most recent call last):
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/runner.py", line 177, in run
    for logs in policy.run():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/basepolicy.py", line 19, in run
    for logs in self.iter():
  File "/home/lizz/parrots/parrots/python/pyparrots/dnn/flowpolicies/iterate.py", line 47, in iter
    self.flow.iterate()
KeyboardInterrupt
